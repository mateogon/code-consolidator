C:\Users\mateo\Desktop\pdf-narrator
C:\Users\mateo\Desktop\pdf-narrator\extract.py
C:\Users\mateo\Desktop\pdf-narrator\generate_audiobook.py
C:\Users\mateo\Desktop\pdf-narrator\generate_audiobook_kokoro.py
C:\Users\mateo\Desktop\pdf-narrator\LICENSE.md
C:\Users\mateo\Desktop\pdf-narrator\main.py
C:\Users\mateo\Desktop\pdf-narrator\README.md
C:\Users\mateo\Desktop\pdf-narrator\requirements.txt
C:\Users\mateo\Desktop\pdf-narrator\test_voices.py
C:\Users\mateo\Desktop\pdf-narrator\ui.py
C:\Users\mateo\Desktop\pdf-narrator\assets
C:\Users\mateo\Desktop\pdf-narrator\assets\demo.mp4
C:\Users\mateo\Desktop\pdf-narrator\assets\demo.png
C:\Users\mateo\Desktop\pdf-narrator\audiobooks
C:\Users\mateo\Desktop\pdf-narrator\Kokoro
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\config.json
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\istftnet.py
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\kokoro.py
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\models.py
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\plbert.py
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\README.md
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\af.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\af_bella.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\af_nicole.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\af_sarah.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\af_sky.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\am_adam.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\am_michael.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\bf_emma.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\bf_isabella.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\bm_george.pt
C:\Users\mateo\Desktop\pdf-narrator\Kokoro\voices\bm_lewis.pt
C:\Users\mateo\Desktop\pdf-narrator\models

#### file: C:\Users\mateo\Desktop\pdf-narrator\extract.py
# extract.py
import fitz
import regex as re
import os

def clean_text(text):
    import regex as re

    # Merge hyphenated line breaks
    text = re.sub(r'-\n\s*', '', text)

    # Normalize spaces around punctuation
    text = re.sub(r'\s*([.,;!?])\s*', r'\1 ', text)

    # Handle quoted content by placing it on its own line
    text = re.sub(r'â€œ([^â€�]*)â€�', r'\nâ€œ\1â€�\n', text)  # For curly quotes
    text = re.sub(r'"([^"]*)"', r'\n"\1"\n', text)  # For straight quotes

    # Split text into lines for processing
    lines = text.splitlines()
    processed_lines = []
    buffer = ""

    for line in lines:
        line = line.strip()
        if not line:
            continue  # Skip empty lines

        if buffer:
            buffer += " " + line
        else:
            buffer = line

        # Check if the buffer ends with punctuation and is not part of a quote
        if re.search(r'[.!?]$', buffer) and not re.search(r'[â€œ"â€�]$', buffer):
            # If there is punctuation within the buffer, split it
            split_buffer = re.split(r'(?<=[.!?])\s+(?![â€�"])', buffer)  # Avoid splitting inside quotes
            processed_lines.extend(split_buffer)
            buffer = ""

    # Add any remaining text in the buffer
    if buffer:
        processed_lines.append(buffer)

    # Collapse excessive blank lines
    processed_lines = [line for line in processed_lines if line.strip()]
    
    # Collapse excessive spaces
    processed_text = "\n".join(processed_lines)
    processed_text = re.sub(r'[ \t]{2,}', ' ', processed_text)

    # Add final newlines after punctuation for TTS readability
    processed_text = re.sub(r'(?<=[.!?])\s*(?!\n)', '\n', processed_text)

    # Remove excessive blank lines
    processed_text = re.sub(r'\n{3,}', '\n\n', processed_text)

    return processed_text.strip()




def extract_cleaned_text(doc, header_threshold=50, footer_threshold=50):
    all_pages = []
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("blocks")
        page_height = page.rect.height
        page_width = page.rect.width

        filtered_lines = []
        for block in blocks:
            x0, y0, x1, y1, text, btype = block[:6]

            # Exclude headers/footers
            if y1 < header_threshold:
                continue
            elif y0 > page_height - footer_threshold:
                continue

            # Exclude page numbers
            if re.match(r'^\d+$', text.strip()):
                continue

            # Exclude very small blocks
            block_width = x1 - x0
            block_height = y1 - y0
            if block_width < 0.1 * page_width and block_height < 0.1 * page_height:
                continue

            filtered_lines.append(text)

        page_text = "\n".join(filtered_lines)
        page_text = clean_text(page_text)
        all_pages.append(page_text)
    return all_pages


def get_table_of_contents(doc):
    toc = doc.get_toc()
    if not toc:
        print("No TOC found in the document.")
        return []
    else:
        print(f"Table of Contents extracted with {len(toc)} entries.")
    return toc

def deduplicate_toc(toc):
    seen_pages = set()
    deduplicated_toc = []
    for entry in toc:
        level, title, page_number = entry
        if page_number not in seen_pages:
            deduplicated_toc.append(entry)
            seen_pages.add(page_number)
        else:
            print(f"Duplicate TOC entry removed: {entry}")
    return deduplicated_toc

def remove_overlap(prev_text, curr_text):
    prev_lines = prev_text.splitlines()
    curr_lines = curr_text.splitlines()
    for overlap_size in range(min(len(prev_lines), len(curr_lines)), 0, -1):
        if prev_lines[-overlap_size:] == curr_lines[:overlap_size]:
            return "\n".join(prev_lines[:-overlap_size])
    return prev_text

def structure_text_by_toc(toc, all_pages_text):
    chapters = []
    last_chapter_text = None
    last_title = None
    last_level = None

    for i, entry in enumerate(toc):
        level, title, start_page = entry
        start_page_idx = start_page - 1

        if i < len(toc) - 1:
            _, _, next_page = toc[i + 1]
            end_page_idx = max(start_page_idx, next_page - 1)
        else:
            end_page_idx = len(all_pages_text) - 1

        chapter_pages = all_pages_text[start_page_idx:end_page_idx]
        chapter_text = clean_text("\n".join(chapter_pages))
        clean_title = title.strip('\r\n')

        if last_chapter_text is not None:
            last_chapter_text = remove_overlap(last_chapter_text, chapter_text)
            chapters.append((last_level, last_title, last_chapter_text))

        last_chapter_text = chapter_text
        last_title = clean_title
        last_level = level

    if last_chapter_text is not None:
        chapters.append((last_level, last_title, last_chapter_text))

    return chapters

def save_chapters(chapters, book_name, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    padding = len(str(len(chapters)))

    for idx, (level, title, text) in enumerate(chapters, 1):
        safe_title = re.sub(r'[^a-zA-Z0-9_\- ]', '', title)
        safe_title = safe_title.strip().replace(' ', '_')
        if not safe_title:
            safe_title = f"chapter_{idx}"

        filename = f"{str(idx).zfill(padding)}_{safe_title}.txt"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(text)

def save_whole_book(book_name, all_pages_text, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_file = os.path.join(output_dir, f"{book_name}_full_text.txt")
    full_text = "\n".join(all_pages_text)
    full_text = clean_text(full_text)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(full_text)
    print(f"All content saved to: {output_file}")

def extract_book(pdf_path, use_toc=True, extract_mode="chapters", output_base_dir="extracted_pdf", progress_callback=None):
    """
    Extract book text from a PDF.
    :param pdf_path: Path to the input PDF file
    :param use_toc: Whether to use TOC if available
    :param extract_mode: "chapters" or "whole"
    :param output_base_dir: Base directory for extraction results
    :return: The output directory containing extracted text files.
    """
    if progress_callback:
        progress_callback(10)
    if not os.path.isfile(pdf_path):
        raise FileNotFoundError(f"File '{pdf_path}' does not exist.")

    doc = fitz.open(pdf_path)
    all_pages_text = extract_cleaned_text(doc)
    toc = get_table_of_contents(doc)
    deduplicated_t = deduplicate_toc(toc)
    book_name = os.path.splitext(os.path.basename(pdf_path))[0]

    # Output directory structure: extracted_pdf/<book_name>/
    output_dir = output_base_dir
    os.makedirs(output_dir, exist_ok=True)

    if use_toc and deduplicated_t and extract_mode == "chapters":
        output_dir = os.path.join(output_base_dir, book_name)
        os.makedirs(output_dir, exist_ok=True)

        chapters = structure_text_by_toc(deduplicated_t, all_pages_text)
        save_chapters(chapters, book_name, output_dir)
    else:
        # Either no TOC or user wants the whole book
        print("Output dir is: ", output_dir)
        save_whole_book(book_name, all_pages_text, output_dir)

    doc.close()
    print("Extraction complete.")
    if progress_callback:
        progress_callback(100)
    return output_dir


#### file: C:\Users\mateo\Desktop\pdf-narrator\generate_audiobook.py
# generate_audiobook.py
import os
import subprocess
import re
import shutil
import time

DEFAULT_TARGET_DURATION = 65  # 1 minute and 5 seconds

# Default voice configs if no speakers are provided
VOICE_CONFIGS = [
    {"id": 380, "length_scale": DEFAULT_TARGET_DURATION/52},
    {"id": 275, "length_scale": DEFAULT_TARGET_DURATION/68},
    {"id": 181, "length_scale": DEFAULT_TARGET_DURATION/79},
    {"id": 859, "length_scale": DEFAULT_TARGET_DURATION/77},
    {"id": 868, "length_scale": DEFAULT_TARGET_DURATION/67},
    {"id": 8, "length_scale": DEFAULT_TARGET_DURATION/63},
]



def split_text_smart(text, chunk_size):
    if len(text) <= chunk_size:
        return [text]

    hierarchy = [
        r'\n\n',
        r'\.\n',
        r'\.',
        r',',
        r':',
        r'\n',
        r' '
    ]

    for pattern in hierarchy:
        split_points = [m.start() for m in re.finditer(pattern, text)]
        split_points = [p for p in split_points if p < chunk_size]

        if split_points:
            cut_point = split_points[-1] + 1
            return [text[:cut_point].strip()] + split_text_smart(text[cut_point:].strip(), chunk_size)

    backtrack_point = text.rfind(' ', 0, chunk_size)
    if backtrack_point == -1:
        backtrack_point = chunk_size
    return [text[:backtrack_point].strip()] + split_text_smart(text[backtrack_point:].strip(), chunk_size)

def generate_audio_chunk(chunk, model_path, output_path, speaker=None, length_scale=1.0, device="cuda"):
    # Build command
    cmd = f'echo "{chunk}" | piper --model "{model_path}" --output_file "{output_path}" --{device} --length-scale {length_scale}'
    if speaker is not None:
        cmd += f' --speaker {speaker}'

    try:
        subprocess.run(cmd, shell=True, check=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"Failed to process chunk. Error: {e}")
        return False

def generate_audiobooks(input_dir, model_path, speaker_ids=None, chunk_size=2500, audio_format=".wav", output_dir=None, progress_callback=None, device="cuda", cancellation_flag=None, update_estimate_callback=None, pause_event=None):
    """
    Generate audiobook from text files in input_dir.
    :param input_dir: Directory containing .txt files.
    :param model_path: Path to the TTS model (.onnx file).
    :param speaker_ids: List of speaker IDs or empty for no speaker param.
    :param chunk_size: Maximum characters per chunk.
    :param audio_format: ".wav" or ".mp3"
    :param output_dir: Directory to store generated audiobook files.
    :return: List of generated audio file paths.
    """
    if not os.path.isdir(input_dir):
        raise FileNotFoundError(f"Input directory '{input_dir}' does not exist.")
    
    if output_dir is None:
        book_name = os.path.basename(os.path.normpath(input_dir))
        output_dir = os.path.join(os.path.dirname(input_dir), f"{book_name}_audio")
        os.makedirs(output_dir, exist_ok=True)

    files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]
    files.sort()
    
    # Calculate total text length
    total_text_length = 0
    for f in files:
        with open(os.path.join(input_dir, f), 'r', encoding='utf-8') as tempf:
            total_text_length += len(tempf.read())
    
    total_characters_processed = 0
    total_time_spent = 0.0

    recent_times = []

    def chunk_done_callback(chars_in_chunk, chunk_duration):
        nonlocal total_characters_processed, total_time_spent, recent_times

        # Update totals
        total_characters_processed += chars_in_chunk
        total_time_spent += chunk_duration

        # Update sliding window of recent times
        recent_times.append(chunk_duration / chars_in_chunk)
        if len(recent_times) > 5:  # Keep only the last 5 records
            recent_times.pop(0)

        # Weighted average of recent and overall averages
        recent_avg_time_per_char = sum(recent_times) / len(recent_times)
        overall_avg_time_per_char = total_time_spent / total_characters_processed
        weighted_avg_time_per_char = (recent_avg_time_per_char * 0.7 + overall_avg_time_per_char * 0.3)

        # Estimate remaining time
        chars_left = total_text_length - total_characters_processed
        remaining_time = weighted_avg_time_per_char * chars_left

        # Clamp remaining time to avoid premature zero
        remaining_time = max(remaining_time, weighted_avg_time_per_char * 100)

        # Pass updated time to the callback
        if update_estimate_callback:
            update_estimate_callback(remaining_time)

    total_files = len(files)
    generated_files = []
    file_counter = 1

    for text_file in files:
        if cancellation_flag and cancellation_flag():
            print("Process canceled before file:", text_file)
            break

        progress = int((file_counter / total_files) * 100)
        if progress_callback:
            progress_callback(progress)

        input_path = os.path.join(input_dir, text_file)
        base_name = os.path.splitext(text_file)[0]
        output_path = os.path.join(output_dir, f"{base_name}{audio_format}")

        print(f"Processing file: {input_path}")
        generate_audio_for_file(input_path, model_path, output_path, chunk_size, speaker_ids, device, cancellation_flag, chunk_done_callback, pause_event)
        generated_files.append(output_path)
        file_counter += 1

    return generated_files

def combine_audio_files(temp_dir, output_path, audio_format=".wav"):
    """
    Combines multiple audio chunks into a single output file.
    :param temp_dir: Directory containing temporary audio chunks.
    :param output_path: Path to the final output file.
    :param audio_format: Desired output format (e.g., ".wav").
    """
    input_file_list = os.path.join(temp_dir, "file_list.txt")
    with open(input_file_list, 'w') as file_list:
        for chunk_file in sorted(os.listdir(temp_dir)):
            if chunk_file.endswith(".wav"):  # Add only WAV files to the list
                chunk_file_path = os.path.join(temp_dir, chunk_file)
                file_list.write(f"file '{chunk_file_path}'\n")

    if audio_format == ".mp3":
        cmd_combine = (
            f'ffmpeg -y -f concat -safe 0 -i "{input_file_list}" '
            f'-vn -ar 44100 -ac 2 -b:a 192k "{output_path}"'
        )
    else:
        # Default to WAV if no transcoding is needed
        cmd_combine = (
            f'ffmpeg -y -f concat -safe 0 -i "{input_file_list}" '
            f'-c copy "{output_path}"'
        )

    print(f"Combining chunks into {output_path}")
    try:
        subprocess.run(cmd_combine, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Failed to combine audio files. Error: {e}")
        raise


def generate_audio_for_file(input_path, model_path, output_path, chunk_size=2500, speaker_ids=None, device="cuda", cancellation_flag=None, chunk_done_callback=None, pause_event=None):
    temp_dir = os.path.join(os.path.dirname(output_path), "temp_chunks")
    os.makedirs(temp_dir, exist_ok=True)

    with open(input_path, 'r', encoding='utf-8') as file:
        text = file.read()

    chunks = split_text_smart(text, chunk_size)
    voice_index = 0
    use_default_voices = (speaker_ids is None or len(speaker_ids) == 0)

    for idx, chunk in enumerate(chunks):
        if cancellation_flag and cancellation_flag():
            print("Process canceled during chunk processing.")
            return
        if pause_event and not pause_event.is_set():
            pause_event.wait()  # Wait until the process is resumed
        if use_default_voices:
            voice = VOICE_CONFIGS[voice_index]
            length_scale = voice["length_scale"]
            speaker = voice["id"]
        else:
            speaker = speaker_ids[idx % len(speaker_ids)]
            length_scale = 1.0

        chunk_output_path = os.path.join(temp_dir, f"chunk_{idx + 1}.wav")
        print(f"Processing chunk {idx + 1}/{len(chunks)} with speaker {speaker}, length_scale {length_scale}, device {device}")

        chunk_start_time = time.time()
        success = generate_audio_chunk(chunk, model_path, chunk_output_path, speaker=speaker, length_scale=length_scale, device=device)
        chunk_end_time = time.time()
        if success and chunk_done_callback:
            chunk_done_callback(len(chunk), chunk_end_time - chunk_start_time)

        if use_default_voices:
            voice_index = (voice_index + 1) % len(VOICE_CONFIGS)

    try:
        combine_audio_files(temp_dir, output_path, os.path.splitext(output_path)[1])
    except Exception as e:
        print(f"Failed to combine audio files. Error: {e}")
    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

    print(f"Audio generation complete: {output_path}")


#### file: C:\Users\mateo\Desktop\pdf-narrator\generate_audiobook_kokoro.py
import os
import time
import numpy as np
import torch
from scipy.io.wavfile import write

# Kokoro imports
from Kokoro.models import build_model
from Kokoro.kokoro import generate


def generate_audio_for_file_kokoro(
    input_path,
    model,
    voicepack,
    output_path,
    device="cuda",
    cancellation_flag=None,
    progress_callback=None,
    pause_event=None,
    max_tokens=510
):
    """
    Read a single .txt file, pass its entire text to Kokoro's `generate`,
    and then write out a single combined audio file.
    No extra chunking is done here; chunking happens inside Kokoro.
    """

    # 1. Read the file text
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # 2. (Optional) check for cancellation or pause
    if cancellation_flag and cancellation_flag():
        print("Process canceled before generating audio.")
        return

    if pause_event and not pause_event.is_set():
        pause_event.wait()

    # 3. Generate audio via Kokoro
    #    We pass `progress_callback` so Kokoro can report chunk-by-chunk progress.
    audio_chunks, phoneme_chunks = generate(
        model=model,
        text=text,
        voicepack=voicepack,
        lang='a',         # Adjust if your voice name implies a different language code
        speed=1,
        max_tokens=max_tokens,
        progress_callback=progress_callback,
        cancellation_flag = cancellation_flag
    )

    # 4. Combine all chunks into one NumPy array
    if not audio_chunks:
        print(f"No audio was generated for file: {input_path}")
        return

    combined_audio = np.concatenate(audio_chunks)

    # 5. Normalize to int16 for WAV
    normalized_audio = (combined_audio / np.max(np.abs(combined_audio)) * 32767).astype('int16')

    # 6. Save as 24 kHz WAV (adjust if your model uses a different sample rate)
    write(output_path, 24000, normalized_audio)
    print(f"Audio saved to {output_path}")


def generate_audiobooks_kokoro(
    input_dir,
    model_path,
    voicepack_path,
    output_dir=None,
    audio_format=".wav",
    progress_callback=None,
    device="cuda",
    cancellation_flag=None,
    update_estimate_callback=None,
    pause_event=None,
    max_tokens=510
):
    """
    Generate audiobooks from .txt files inside `input_dir`, using Kokoro TTS.
    """

    # 1. Validate input
    if not os.path.isdir(input_dir):
        raise FileNotFoundError(f"Input directory '{input_dir}' does not exist.")

    # 2. Determine output directory
    if output_dir is None:
        book_name = os.path.basename(os.path.normpath(input_dir))
        output_dir = os.path.join(os.path.dirname(input_dir), f"{book_name}_audio")
    os.makedirs(output_dir, exist_ok=True)

    # 3. Get .txt files
    files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]
    files.sort()
    total_files = len(files)

    # 4. Load Kokoro model + voicepack
    device = device if (torch.cuda.is_available() and device == "cuda") else "cpu"
    print(f"Loading model from: {model_path} (device={device})")
    MODEL = build_model(model_path, device)

    print(f"Loading voicepack from: {voicepack_path}")
    VOICEPACK = torch.load(voicepack_path, weights_only=True).to(device)

    # 5. Measure total text length for time estimates
    total_text_length = 0
    for f in files:
        with open(os.path.join(input_dir, f), 'r', encoding='utf-8') as tempf:
            total_text_length += len(tempf.read())

    total_characters_processed = 0
    total_time_spent = 0.0
    recent_times = []  # Sliding window for recent chunk times

    # 6. Define an internal chunk callback for Kokoro
    def kokoro_chunk_done_callback(chars_in_chunk, chunk_duration):
        """
        Called by Kokoro for each chunk; used to track time & update estimates.
        """
        nonlocal total_characters_processed, total_time_spent, recent_times

        # Update total stats
        total_characters_processed += chars_in_chunk
        total_time_spent += chunk_duration

        # Track recent chunk durations
        time_per_char = chunk_duration / chars_in_chunk if chars_in_chunk > 0 else 0
        recent_times.append(time_per_char)
        if len(recent_times) > 5:  # Keep the last 5 times
            recent_times.pop(0)

        # Calculate weighted average
        recent_avg = sum(recent_times) / len(recent_times) if recent_times else 0
        overall_avg = total_time_spent / total_characters_processed if total_characters_processed else 0
        weighted_avg = 0.7 * recent_avg + 0.3 * overall_avg

        # Estimate remaining time
        chars_left = total_text_length - total_characters_processed
        remaining_time = weighted_avg * chars_left

        if update_estimate_callback:
            update_estimate_callback(max(remaining_time, 0))  # Ensure no negative time

    # 7. Process each file
    generated_files = []
    for i, text_file in enumerate(files, start=1):
        # Check for cancellation
        if cancellation_flag and cancellation_flag():
            print("Process canceled before file:", text_file)
            break

        # File-level progress
        progress_percent = int((i / total_files) * 100)
        if progress_callback:
            progress_callback(progress_percent)

        input_path = os.path.join(input_dir, text_file)
        base_name = os.path.splitext(text_file)[0]
        output_path = os.path.join(output_dir, f"{base_name}{audio_format}")
        print(f"\n=== Generating audio for: {input_path} ===")
        print(f"Output file: {output_path}")
        generate_audio_for_file_kokoro(
            input_path=input_path,
            model=MODEL,
            voicepack=VOICEPACK,
            output_path=output_path,
            device=device,
            cancellation_flag=cancellation_flag,
            progress_callback=kokoro_chunk_done_callback,  # Pass chunk callback to Kokoro
            pause_event=pause_event,
            max_tokens=max_tokens
        )

        generated_files.append(output_path)

    return generated_files


#### file: C:\Users\mateo\Desktop\pdf-narrator\LICENSE.md
MIT License

Copyright (c) 2024 Mateo Gonzalez Arrigorriaga

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


#### file: C:\Users\mateo\Desktop\pdf-narrator\main.py
# main.py
import ttkbootstrap as tb
from ui import AudiobookApp
def main():
    app = AudiobookApp()
    app.mainloop()

if __name__ == "__main__":
    main()

#### file: C:\Users\mateo\Desktop\pdf-narrator\README.md
# PDF Narrator (Kokoro Edition)

Transform your PDF documents into audiobooks effortlessly using **advanced text extraction** and **Kokoro TTS** technology. This fork/variation of Kokoro allows for **longer file generation** and better handling of extracted PDF text.

## Demo

1. **Screenshot**  
   Check out the GUI in the screenshot below:  
   ![Demo Screenshot](assets/demo.png)

2. **Audio Sample**  
   Listen to a short sample of the generated audiobook:  
   https://github.com/user-attachments/assets/02953345-aceb-41f3-babf-1d1606c76641

## Features

- **Intelligent PDF Text Extraction**

  - Skips headers, footers, and page numbers.
  - Optionally splits based on Table of Contents (TOC) or extracts the entire document.

- **Kokoro TTS Integration**

  - Generate natural-sounding audiobooks with the [Kokoro-82M model](https://huggingface.co/hexgrad/Kokoro-82M).
  - Easily select or swap out different `.pt` voicepacks.

- **User-Friendly GUI**

  - Modern interface with **ttkbootstrap** (theme selector, scrolled logs, progress bars).
  - Pause/resume and cancel your audiobook generation anytime.

- **Configurable for Low-VRAM Systems**
  - Choose the chunk size for text to accommodate limited GPU resources.
  - Switch to CPU if no GPU is available.

---

## Prerequisites

- **Python 3.8+**
- **FFmpeg** (for audio-related tasks on some systems).
- **Torch** (PyTorch for the Kokoro TTS model).
- **Other Dependencies** listed in `requirements.txt`.

---

## Installation

1. **Clone the Repository**

   ```bash
   git clone https://github.com/mateogon/pdf-narrator.git
   cd pdf-narrator
   ```

2. **Create and Activate a Virtual Environment**

   ```bash
   python -m venv venv
   # On Linux/macOS:
   source venv/bin/activate
   # On Windows:
   venv\Scripts\activate
   ```

3. **Install Python Dependencies**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Download Kokoro Model**

   - Go to the [Kokoro-82M Hugging Face page](https://huggingface.co/hexgrad/Kokoro-82M).
   - Download the model checkpoint:  
     [kokoro-v0_19.pth?download=true](https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/kokoro-v0_19.pth?download=true)
   - Place this file in the `models/` directory (or a subdirectory) of your project.  
     Example:
     ```bash
     mkdir -p models
     mv /path/to/kokoro-v0_19.pth models/
     ```

5. **Optional: Download Additional Voicepacks**

   - By default, `.pt` files (voicepacks) are in `Kokoro/voices/`.
   - If you have custom voicepacks, place them in `voices/your_custom_file.pt`.

6. **Install FFmpeg** (if you need transcoding/combining WAV files)

   - **Ubuntu/Debian**:
     ```bash
     sudo apt-get install ffmpeg
     ```
   - **macOS**:
     ```bash
     brew install ffmpeg
     ```
   - **Windows**:  
     Download from the [FFmpeg official site](https://ffmpeg.org/download.html) and follow the installation instructions.

---

## Windows Installation Notes

For Windows, certain libraries such as `DeepSpeed`, `lxml`, and `eSpeak NG` may require special steps for installation. Follow these guidelines to ensure a smooth setup.

### 1. **Prerequisites**

- **Python 3.12.7**  
  Download and install [Python 3.12.7](https://www.python.org/downloads/).  
  Ensure `python` and `pip` are added to your system's PATH during installation.

- **CUDA 12.4** (for GPU acceleration)  
  Install the [CUDA 12.4 Toolkit](https://developer.nvidia.com/cuda-downloads) to ensure compatibility with precompiled DeepSpeed.

### 2. **Installing eSpeak NG**

eSpeak NG is a lightweight and versatile text-to-speech engine required for phoneme-based operations.

1. **Download the Installer**  
   [https://github.com/espeak-ng/espeak-ng/releases/download/1.51/espeak-ng-X64.msi](https://github.com/espeak-ng/espeak-ng/releases/download/1.51/espeak-ng-X64.msi)

2. **Run the Installer**

   - Double-click the `.msi` file to start the installation.
   - Follow the on-screen instructions to complete the setup.

3. **Set Environment Variables**  
   Add the following environment variables for `phonemizer` compatibility:

   - `PHONEMIZER_ESPEAK_LIBRARY`  
     `C:\Program Files\eSpeak NG\libespeak-ng.dll`

   - `PHONEMIZER_ESPEAK_PATH`  
     `C:\Program Files (x86)\eSpeak\command_line\espeak.exe`

   **Steps to Add Environment Variables**:

   - Right-click on "This PC" or "Computer" and select "Properties".
   - Go to "Advanced system settings" > "Environment Variables".
   - Under "System variables", click "New" and add the variables above with their respective values.
   - Click "OK" to save the changes.

4. **Verify Installation**
   - Open Command Prompt and check the version of `eSpeak NG`:
     ```cmd
     espeak-ng --version
     ```

### 3. **Using Precompiled Wheels for DeepSpeed and lxml**

1. **Download Wheels**

   - **DeepSpeed** (Python 3.12.7, CUDA 12.4)  
     [https://huggingface.co/NM156/deepspeed_wheel/tree/main](https://huggingface.co/NM156/deepspeed_wheel/tree/main)

   - **lxml** (Python 3.12)  
     [https://github.com/lxml/lxml/releases/tag/lxml-5.3.0](https://github.com/lxml/lxml/releases/tag/lxml-5.3.0)

2. **Install the Wheels**  
   Activate your virtual environment and install the downloaded wheels:

   ```cmd
   # Activate the virtual environment
   venv\Scripts\activate

   # Install DeepSpeed
   pip install path\to\deepspeed-0.11.2+cuda124-cp312-cp312-win_amd64.whl

   # Install lxml
   pip install path\to\lxml-5.3.0-cp312-cp312-win_amd64.whl
   ```

### 4. **Verify Installation**

Once installed, verify the tools and libraries:

```cmd
# Check DeepSpeed version
deepspeed --version

# Check lxml installation
pip show lxml

# Check eSpeak NG version
espeak-ng --version
```

### 5. **Optional: Compile Libraries Yourself**

If youâ€™re using a different Python or CUDA version, or if the precompiled wheels donâ€™t match your environment, you may need to compile `DeepSpeed` and `lxml` yourself. Refer to the steps in the DeepSpeed documentation or each libraryâ€™s GitHub for detailed build instructions.

---

## Quick Start

1. **Launch the App**

   ```bash
   python main.py
   ```

2. **Select a Mode**

   - **Single PDF**: Choose a specific PDF file and extract its text.
   - **Batch PDFs**: Select a folder with multiple PDFs. The app processes all PDFs in the folder (and subfolders).
   - **Skip Extraction**: Use pre-extracted text files. The app retains the folder structure for audiobook generation.

3. **Extract Text (for Single/Batch Modes)**

   - If TOC is available, extract by chapters. Otherwise, extract the entire book.
   - For batch processing, the app maintains the relative folder structure for all PDFs.

4. **Configure Kokoro TTS Settings**

   - Select the `.pth` model (e.g., `models/kokoro-v0_19.pth`).
   - Pick a `.pt` voicepack (e.g., `voices/af_sarah.pt`).
   - Adjust chunk size if you have limited VRAM.
   - Choose output audio format (`.wav` or `.mp3`).

5. **Generate Audiobook**

   - Click **Start Process**.
   - Track progress via logs, estimated time, and progress bars.
   - Pause/Resume or Cancel at any point.

6. **Enjoy Your Audiobook**

   - Open the output folder to find your generated `.wav` or `.mp3` files.

---

## Technical Highlights

### PDF Extraction

- Built atop [PyMuPDF](https://pymupdf.readthedocs.io/) for parsing text.
- Cleans up headers, footers, page numbers, and multi-hyphen lines.
- _Chapters vs. Whole:_
  - If TOC is found, you can split into smaller .txt files.
  - Otherwise, extract the entire text into one file.

#### **Three Modes for PDF/Text Processing**

1. **Single PDF**

   - Extract text from one PDF file.
   - Output directory: `extracted_pdf/<book_name>`.

2. **Batch PDFs**

   - Recursively process all PDFs in a selected folder.
   - Maintains folder structure under `extracted_pdf/`.

3. **Skip Extraction**
   - Use pre-extracted text files organized in folders.
   - Input folder structure is mirrored for audiobook output.

### Kokoro TTS

- **Text Normalization & Phonemization**  
  Built-in text normalization for years, times, currency, etc.

- **Token-Based Splitting**  
  Splits text into < 510 tokens per chunk to accommodate model constraints.  
  Joins all chunked audio into a single final file.

- **Voicepacks (.pt)**  
  Each voicepack provides a reference embedding for a given voice.

### Low-VRAM/Speed Tips

- **Chunk Size**  
  If you run out of GPU memory, lower your chunk size from the default (2500) to something smaller (e.g., 1000 or 500).

- **Device Selection**  
  Choose `CUDA` if you have a compatible GPU, or `CPU` for CPU-only systems.

---

## Limitations

1. **PDF Layout**  
   Extraction can vary if the PDF has complex formatting or unusual text flow.

2. **TTS Quality**  
   The generated speech depends on the **Kokoro** modelâ€™s training and quality.

3. **Processing Time**  
   Long PDFs with complex text can take a while to extract and convert.

---

## Contributing

We welcome contributions!

- Fork, branch, and submit a pull request.
- Report bugs via [Issues](https://github.com/mateogon/pdf-narrator/issues).

---

## License

This project is released under the [MIT License](LICENSE.md).

Enjoy converting your PDFs into immersive audiobooks powered by **Kokoro** TTS!


#### file: C:\Users\mateo\Desktop\pdf-narrator\requirements.txt
attrs==24.3.0
babel==2.16.0
beautifulsoup4==4.12.3
bibtexparser==2.0.0b8
bs4==0.0.2
certifi==2024.12.14
charset-normalizer==3.4.1
click==8.1.8
clldutils==3.24.0
colorama==0.4.6
colorlog==6.9.0
csvw==3.5.1
dlinfo==1.2.1
filelock==3.16.1
fsspec==2024.12.0
huggingface-hub==0.27.0
idna==3.10
isodate==0.7.2
Jinja2==3.1.5
joblib==1.4.2
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
language-tags==1.2.0
lxml==5.3.0
Markdown==3.7
MarkupSafe==3.0.2
mpmath==1.3.0
munch==4.0.0
networkx==3.4.2
numpy==2.2.1
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
packaging==24.2
phonemizer==3.3.0
pillow==11.1.0
pylatexenc==2.10
PyMuPDF==1.25.1
pyparsing==3.2.1
PyQt6==6.8.0
PyQt6-Qt6==6.8.1
PyQt6_sip==13.9.1
python-dateutil==2.9.0.post0
PyYAML==6.0.2
rdflib==7.1.1
referencing==0.35.1
regex==2024.11.6
requests==2.32.3
rfc3986==1.5.0
rpds-py==0.22.3
safetensors==0.5.0
scipy==1.15.0
segments==2.2.1
six==1.17.0
soupsieve==2.6
sympy==1.13.1
tabulate==0.9.0
tokenizers==0.21.0
torch==2.5.1
tqdm==4.67.1
transformers==4.47.1
triton==3.1.0
ttkbootstrap==1.10.1
typing_extensions==4.12.2
uritemplate==4.1.1
urllib3==2.3.0


#### file: C:\Users\mateo\Desktop\pdf-narrator\test_voices.py
from Kokoro.models import build_model
import torch
from scipy.io.wavfile import write
import numpy as np
from Kokoro.kokoro import generate

# Select device
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load the model
MODEL = build_model('/home/mateo/Desktop/PdfExtract/models/kokoro-v0_19.pth', device)

# List of voice names to generate audio for
VOICE_NAMES = ['am_adam','af_sarah','af','af_sky']  # Add desired voice names here

# Read the text to synthesize from a file
input_text_file = "/home/mateo/Desktop/PdfExtract/test_text.txt"  # Path to your text file
with open(input_text_file, 'r') as file:
    text = file.read()

# Loop through each voice and generate audio
for voice_name in VOICE_NAMES:
    try:
        # Load the voicepack
        voicepack_path = f'Kokoro/voices/{voice_name}.pt'
        VOICEPACK = torch.load(voicepack_path, weights_only=True).to(device)
        print(f"Loaded voice: {voice_name}")

        # Generate audio
        audio_chunks, phoneme_chunks = generate(MODEL, text, VOICEPACK, lang=voice_name[0])

        # Combine and save the normalized audio
        combined_audio = np.concatenate(audio_chunks)
        normalized_audio = (combined_audio / np.max(np.abs(combined_audio)) * 32767).astype('int16')
        output_path = f"output_audio_{voice_name}.wav"
        write(output_path, 24000, normalized_audio)
        print(f"Audio saved to {output_path}")

        # Debug: Print audio stats
        print(f"Audio stats for {voice_name}:")
        print("  Audio waveform preview:", combined_audio[:10])  # Show the first 10 samples
        print("  Max amplitude:", max(combined_audio), "Min amplitude:", min(combined_audio))
        print("  Audio data type:", combined_audio.dtype)

    except Exception as e:
        print(f"Failed to generate audio for {voice_name}: {e}")


#### file: C:\Users\mateo\Desktop\pdf-narrator\ui.py
# ui.py

import tkinter as tk
from tkinter import filedialog
from PyQt6.QtWidgets import QApplication, QFileDialog
from tkinter import scrolledtext
import ttkbootstrap as tb
from ttkbootstrap.constants import *
import os
from extract import extract_book
# Import only Kokoro's audiobook generator
from generate_audiobook_kokoro import generate_audiobooks_kokoro
import sys
import threading
import time
import json


class LogRedirector:
    def __init__(self, write_callback):
        self.write_callback = write_callback
        self.is_logging = False  # Prevent recursion

    def write(self, message):
        if self.is_logging:  # Avoid recursive logging
            return

        self.is_logging = True
        try:
            if message.strip():  # Avoid empty messages
                self.write_callback(message)
        finally:
            self.is_logging = False  # Reset flag

    def flush(self):
        pass  # Not needed for tkinter text widgets


class SourceFrame(tb.Frame):
    """
    Frame for PDF source selection and extraction options.
    """
    def __init__(self, master, *args, **kwargs):
        super().__init__(master, *args, **kwargs)

        # Variables
        self.project_dir = os.path.dirname(os.path.abspath(__file__))  # Project directory

        # For single PDF or batch folder
        self.pdf_path = tk.StringVar()
        self.pdf_folder = tk.StringVar()

        # For skipping extraction (manual extracted folder)
        self.manual_extracted_dir = tk.StringVar()

        # Extracted text options
        self.extracted_text_dir = tk.StringVar()  # Directory for extracted text
        self.use_toc = tk.BooleanVar(value=True)
        self.extract_mode = tk.StringVar(value="chapters")  # "chapters" or "whole"

        # Radio variable to decide between modes: single, batch, or skip extraction
        self.source_option = tk.StringVar(value="single")  

        # Title
        source_label = tb.Label(
            self, 
            text="PDF Source & Extraction", 
            style="Secondary.TLabel", 
            font="-size 14 -weight bold"
        )
        source_label.pack(pady=10)

        # Source option radio buttons
        source_option_frame = tb.Labelframe(self, text="Choose Source Option")
        source_option_frame.pack(fill=tk.X, pady=5, padx=5)

        tb.Radiobutton(
            source_option_frame, 
            text="Single PDF", 
            variable=self.source_option, 
            value="single",
            command=self._update_ui
        ).pack(anchor=tk.W, padx=5, pady=2)

        tb.Radiobutton(
            source_option_frame, 
            text="Batch PDFs (select folder)", 
            variable=self.source_option, 
            value="batch",
            command=self._update_ui
        ).pack(anchor=tk.W, padx=5, pady=2)

        tb.Radiobutton(
            source_option_frame, 
            text="Skip Extraction (use existing text folder)", 
            variable=self.source_option, 
            value="skip",
            command=self._update_ui
        ).pack(anchor=tk.W, padx=5, pady=2)

        # Single PDF selection
        single_frame = tb.Frame(self)
        single_frame.pack(pady=5, fill=tk.X)
        self.single_frame = single_frame  # For toggling

        tb.Label(single_frame, text="Select PDF File (Single):").pack(side=tk.LEFT, padx=5)
        tb.Entry(single_frame, textvariable=self.pdf_path, state=tk.NORMAL).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        tb.Button(single_frame, text="Browse", command=self._browse_single_pdf).pack(side=tk.LEFT, padx=5)

        # Batch PDF folder selection
        batch_frame = tb.Frame(self)
        batch_frame.pack(pady=5, fill=tk.X)
        self.batch_frame = batch_frame  # For toggling

        tb.Label(batch_frame, text="Select Folder (Batch):").pack(side=tk.LEFT, padx=5)
        tb.Entry(batch_frame, textvariable=self.pdf_folder, state=tk.NORMAL).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        tb.Button(batch_frame, text="Browse", command=self._browse_pdf_folder).pack(side=tk.LEFT, padx=5)

        # Manual extracted folder (skip extraction)
        skip_frame = tb.Frame(self)
        skip_frame.pack(pady=5, fill=tk.X)
        self.skip_frame = skip_frame  # For toggling

        tb.Label(skip_frame, text="Existing Text Folder:").pack(side=tk.LEFT, padx=5)
        tb.Entry(skip_frame, textvariable=self.manual_extracted_dir, state=tk.NORMAL).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        tb.Button(skip_frame, text="Browse", command=self._browse_extracted_folder).pack(side=tk.LEFT, padx=5)

        # TOC & Extraction Mode
        options_frame = tb.Labelframe(self, text="Extraction Options (only when extracting)")
        options_frame.pack(fill=tk.X, pady=10, padx=5)

        toc_check = tb.Checkbutton(options_frame, text="Use TOC (if available)", variable=self.use_toc)
        toc_check.pack(anchor=tk.W, padx=5, pady=5)
        
        mode_frame = tb.Frame(options_frame)
        mode_frame.pack(anchor=tk.W, padx=5, pady=5)
        tb.Radiobutton(mode_frame, text="Extract by Chapters", variable=self.extract_mode, value="chapters").pack(side=tk.LEFT)
        tb.Radiobutton(mode_frame, text="Extract Whole Book", variable=self.extract_mode, value="whole").pack(side=tk.LEFT, padx=10)

        # Extracted Text Directory (auto-determined if single or batch PDF is used)
        out_frame = tb.Frame(self)
        out_frame.pack(pady=5, fill=tk.X)
        
        tb.Label(out_frame, text="Extracted Text Directory (auto):").pack(side=tk.LEFT, padx=5)
        tb.Entry(out_frame, textvariable=self.extracted_text_dir, state=tk.NORMAL).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)

        # Initialize UI
        self._update_ui()

    def _update_ui(self):
        """
        Enable/disable controls based on the selected source option.
        """
        option = self.source_option.get()

        # Toggle visibility based on the mode
        if option == "single":
            self._toggle_frame(self.single_frame, enable=True)
            self._toggle_frame(self.batch_frame, enable=False)
            self._toggle_frame(self.skip_frame, enable=False)
        elif option == "batch":
            self._toggle_frame(self.single_frame, enable=False)
            self._toggle_frame(self.batch_frame, enable=True)
            self._toggle_frame(self.skip_frame, enable=False)
        elif option == "skip":
            self._toggle_frame(self.single_frame, enable=False)
            self._toggle_frame(self.batch_frame, enable=False)
            self._toggle_frame(self.skip_frame, enable=True)

    def _toggle_frame(self, frame, enable=True):
        """
        Enable or disable all widgets in a frame.
        """
        state = tk.NORMAL if enable else tk.DISABLED
        for child in frame.winfo_children():
            child.configure(state=state)

    def _browse_single_pdf(self):
        # Initialize or reuse the existing QApplication instance
        qt_app = QApplication.instance() or QApplication(sys.argv)

        # Open QFileDialog to select a single PDF file
        path, _ = QFileDialog.getOpenFileName(
            None,
            "Select PDF File",
            self.project_dir,
            "PDF Files (*.pdf)"
        )
        if path:  # If a file was selected
            self.pdf_path.set(path)

            # Auto-populate extracted_text_dir for the single PDF
            book_name = os.path.splitext(os.path.basename(path))[0]
            extracted_text_dir = os.path.join(self.project_dir, "extracted_pdf", book_name)
            self.extracted_text_dir.set(extracted_text_dir)

            # Update the audiobook output folder in the parent app
            if hasattr(self.master.master, 'audio_frame'):  # Ensure parent has the audio_frame attribute
                self.master.master.audio_frame.update_audio_output_dir(book_name)


    def _browse_pdf_folder(self):
        # Initialize or reuse the existing QApplication instance
        qt_app = QApplication.instance() or QApplication(sys.argv)

        # Open QFileDialog to select a folder
        folder = QFileDialog.getExistingDirectory(
            None,
            "Select Folder Containing PDFs",
            self.project_dir
        )
        if folder:  # If a folder was selected
            self.pdf_folder.set(folder)

            # Get the last part of the folder name
            folder_name = os.path.basename(folder.rstrip(os.sep))

            # Set extracted_text_dir as the base output folder for batch mode
            extracted_text_dir = os.path.join(self.project_dir, "extracted_pdf", folder_name)
            self.extracted_text_dir.set(extracted_text_dir)

            # Update the audiobook output folder in the parent app
            if hasattr(self.master.master, 'audio_frame'):  # Ensure parent has the audio_frame attribute
                self.master.master.audio_frame.update_audio_output_dir(folder_name)


    def _browse_extracted_folder(self):
        # Initialize or reuse the existing QApplication instance
        qt_app = QApplication.instance() or QApplication(sys.argv)

        # Open QFileDialog to select a folder
        folder = QFileDialog.getExistingDirectory(
            None,
            "Select Existing Text Folder",
            self.project_dir
        )
        if folder:  # If a folder was selected
            self.manual_extracted_dir.set(folder)

            # Update the audiobook output folder in the parent app
            if hasattr(self.master.master, 'audio_frame'):  # Ensure parent has the audio_frame attribute
                book_name = os.path.basename(folder.rstrip(os.sep))  # Get the last folder name
                self.master.master.audio_frame.update_audio_output_dir(book_name)


    # Methods to get user selections
    def get_source_option(self):
        return self.source_option.get()

    def get_pdf_path(self):
        return self.pdf_path.get()

    def get_pdf_folder(self):
        return self.pdf_folder.get()

    def get_manual_extracted_dir(self):
        return self.manual_extracted_dir.get()

    def get_extracted_text_dir(self):
        return self.extracted_text_dir.get()

    def get_use_toc(self):
        return self.use_toc.get()

    def get_extract_mode(self):
        return self.extract_mode.get()

class AudioFrame(tb.Frame):
    """
    Frame for Kokoro model and voicepack selection + audiobook settings.
    """
    def __init__(self, master, *args, **kwargs):
        super().__init__(master, *args, **kwargs)
        
        # Variables
        self.project_dir = os.path.dirname(os.path.abspath(__file__))  # Project directory

        # Default Kokoro model checkpoint
        self.model_path = tk.StringVar(value="models/kokoro-v0_19.pth")
        # Default voicepack (adjust if you like a different default)
        self.voicepack_path = tk.StringVar(value="Kokoro/voices/af_sarah.pt")

        self.chunk_size = tk.IntVar(value=510)    # Default chunk size
        self.audio_format = tk.StringVar(value=".wav")  # Default audio format
        self.audio_output_dir = tk.StringVar()  # Directory for audiobook files
        self.device = tk.StringVar(value="cuda")  # Default to GPU

        # Title
        audio_label = tb.Label(
            self, 
            text="Kokoro Audio Settings", 
            style="Secondary.TLabel", 
            font="-size 14 -weight bold"
        )
        audio_label.pack(pady=10)

        # Model Selection
        model_frame = tb.Frame(self)
        model_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(model_frame, text="Kokoro Model:").pack(side=LEFT, padx=5)
        models = self._get_pth_files()  # We'll scan for .pth
        model_combo = tb.Combobox(
            model_frame, 
            textvariable=self.model_path, 
            values=models, 
            state="readonly"
        )
        model_combo.pack(side=LEFT, fill=X, expand=True, padx=5)

        # Voicepack Selection
        voicepack_frame = tb.Frame(self)
        voicepack_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(voicepack_frame, text="Voicepack (.pt):").pack(side=LEFT, padx=5)
        voicepacks = self._get_pt_files()  # We'll scan for .pt
        voicepack_combo = tb.Combobox(
            voicepack_frame,
            textvariable=self.voicepack_path,
            values=voicepacks,
            state="readonly"
        )
        voicepack_combo.pack(side=LEFT, fill=X, expand=True, padx=5)

        # Chunk Size
        chunk_frame = tb.Frame(self)
        chunk_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(chunk_frame, text="Chunk Size (tokens):").pack(side=LEFT, padx=5)
        tb.Spinbox(chunk_frame, from_=500, to=5000, increment=500, textvariable=self.chunk_size, width=7).pack(side=LEFT, padx=5)

        # Output Directory
        output_frame = tb.Frame(self)
        output_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(output_frame, text="Audiobook Output Folder:").pack(side=LEFT, padx=5)
        tb.Entry(output_frame, textvariable=self.audio_output_dir, state=READONLY).pack(side=LEFT, fill=X, expand=True, padx=5)

        # Audio Format
        format_frame = tb.Frame(self)
        format_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(format_frame, text="Output Format:").pack(side=LEFT, padx=5)
        formats = [".wav", ".mp3"]
        format_combo = tb.Combobox(
            format_frame, 
            textvariable=self.audio_format, 
            values=formats, 
            state="readonly"
        )
        format_combo.pack(side=LEFT, fill=X, expand=True, padx=5)

        # Device Selection
        device_frame = tb.Frame(self)
        device_frame.pack(fill=X, pady=5, padx=5)

        tb.Label(device_frame, text="Device:").pack(side=LEFT, padx=5)
        tb.Radiobutton(device_frame, text="GPU (CUDA)", variable=self.device, value="cuda").pack(side=LEFT, padx=5)
        tb.Radiobutton(device_frame, text="CPU", variable=self.device, value="cpu").pack(side=LEFT, padx=5)

    def _get_pth_files(self):
        """
        Scan the 'models' directory in the project root for .pth files (Kokoro).
        Returns a list of relative paths to the found models.
        """
        models_dir = os.path.join(self.project_dir, "models")
        model_files = []

        for root, dirs, files in os.walk(models_dir):
            for file in files:
                if file.endswith(".pth"):
                    relative_path = os.path.relpath(os.path.join(root, file), self.project_dir)
                    model_files.append(relative_path)

        return model_files

    def _get_pt_files(self):
        """
        Scan the 'voices' directory for .pt voicepacks (Kokoro).
        Returns a list of relative paths to the found voicepacks.
        """
        voices_dir = os.path.join(self.project_dir, "Kokoro/voices/")
        voice_files = []

        for root, dirs, files in os.walk(voices_dir):
            for file in files:
                if file.endswith(".pt"):
                    relative_path = os.path.relpath(os.path.join(root, file), self.project_dir)
                    voice_files.append(relative_path)

        return voice_files

    def get_device(self):
        return self.device.get()

    def update_audio_output_dir(self, book_name):
        """
        Dynamically update the audiobook output folder based on the book name.
        """
        if book_name:
            audio_output_dir = os.path.join(self.project_dir, "audiobooks", book_name)
            os.makedirs(audio_output_dir, exist_ok=True)
            self.audio_output_dir.set(audio_output_dir)
        else:
            self.audio_output_dir.set("")

    def get_audio_output_dir(self):
        return self.audio_output_dir.get()

    def get_model_path(self):
        """
        Returns the absolute path to the Kokoro .pth model file
        """
        selected_model = self.model_path.get()
        return os.path.join(self.project_dir, selected_model) if selected_model else ""

    def get_voicepack_path(self):
        """
        Returns the absolute path to the Kokoro .pt voicepack file
        """
        selected_vp = self.voicepack_path.get()
        return os.path.join(self.project_dir, selected_vp) if selected_vp else ""

    def get_chunk_size(self):
        return self.chunk_size.get()

    def get_audio_format(self):
        return self.audio_format.get()


class ProgressFrame(tb.Frame):
    """
    Frame for showing progress bars, logs, and controlling start/pause/cancel.
    """
    def __init__(self, master, app, *args, **kwargs):
        super().__init__(master, *args, **kwargs)
        self.app = app  # Reference to main AudiobookApp
        self.pause_event = threading.Event()
        self.pause_event.set()

        # Extraction progress
        self.extract_progress = tk.DoubleVar(value=0.0)
        # Generation progress
        self.audio_progress = tk.DoubleVar(value=0.0)
        self.status_text = tk.StringVar(value="Waiting...")
        
        # Estimated time label
        self.estimated_time_text = tk.StringVar(value="Estimated time remaining: N/A")
        
        # Title
        prog_label = tb.Label(
            self, 
            text="Progress & Logs", 
            style="Secondary.TLabel", 
            font="-size 14 -weight bold"
        )
        prog_label.pack(pady=10)

        # Status
        status_frame = tb.Frame(self)
        status_frame.pack(fill=X, pady=5, padx=5)
        
        tb.Label(status_frame, text="Status:").pack(side=LEFT, padx=5)
        tb.Label(status_frame, textvariable=self.status_text).pack(side=LEFT, padx=5)
        
        # Estimated time label
        tb.Label(status_frame, textvariable=self.estimated_time_text).pack(side=LEFT, padx=15)

        # Progress Bars
        pb_frame = tb.Frame(self)
        pb_frame.pack(fill=X, pady=5, padx=5)
        
        tb.Label(pb_frame, text="Text Extraction:").pack(anchor=W)
        tb.Progressbar(pb_frame, variable=self.extract_progress, maximum=100).pack(fill=X, pady=2)

        tb.Label(pb_frame, text="Audio Generation:").pack(anchor=W, pady=(10, 0))
        tb.Progressbar(pb_frame, variable=self.audio_progress, maximum=100).pack(fill=X, pady=2)

        self.percentage_text = tk.StringVar(value="0% complete")
        tb.Label(pb_frame, textvariable=self.percentage_text).pack(anchor=W, pady=(5, 0))

        # Logs
        log_frame = tb.Labelframe(self, text="Logs")
        log_frame.pack(fill=BOTH, expand=True, pady=5, padx=5)
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=8)
        self.log_text.pack(fill=BOTH, expand=True)

        # Redirect stdout and stderr to the UI
        sys.stdout = LogRedirector(self.log_message)
        sys.stderr = LogRedirector(self.log_message)

        # Action Buttons
        btn_frame = tb.Frame(self)
        btn_frame.pack(pady=10)
        
        self.start_button = tb.Button(btn_frame, text="Start Process", bootstyle=SUCCESS, command=self._start_process_thread)
        self.start_button.pack(side=LEFT, padx=5)

        self.cancel_button = tb.Button(btn_frame, text="Cancel", bootstyle=DANGER, command=self._cancel_process, state=DISABLED)
        self.cancel_button.pack(side=LEFT, padx=5)
        
        self.pause_button = tb.Button(btn_frame, text="Pause", bootstyle=WARNING, command=self._pause_process)
        self.pause_button.pack(side=LEFT, padx=5)

        self.resume_button = tb.Button(btn_frame, text="Resume", bootstyle=INFO, command=self._resume_process, state=DISABLED)
        self.resume_button.pack(side=LEFT, padx=5)
        
        self.cancellation_flag = False
        self.process_thread = None
        self.running = False

    def _start_process_thread(self):
        # Start the process in a background thread
        if self.process_thread and self.process_thread.is_alive():
            self.log_message("A process is already running.")
            return

        self.running = True
        self.cancel_button.config(state=NORMAL)
        self.start_button.config(state=DISABLED)
        self.process_thread = threading.Thread(target=self._start_process, daemon=True)
        self.process_thread.start()

    def _start_process(self):
        self.log_message("Starting process...")
        self.set_status("Extracting text...")
        self.update_extract_progress(10)
        self.cancellation_flag = False  # Reset cancellation flag

        try:
            source_option = self.app.source_frame.get_source_option()
            use_toc = self.app.source_frame.get_use_toc()
            extract_mode = self.app.source_frame.get_extract_mode()
            source_folder = self.app.source_frame.get_pdf_folder()  # Batch source folder
            extracted_root = self.app.source_frame.get_extracted_text_dir()  # Base for extracted output
            manual_extracted_dir = self.app.source_frame.get_manual_extracted_dir()

            model_path = self.app.audio_frame.get_model_path()
            voicepack_path = self.app.audio_frame.get_voicepack_path()
            chunk_size = self.app.audio_frame.get_chunk_size()
            audio_format = self.app.audio_frame.get_audio_format()
            audio_root = self.app.audio_frame.get_audio_output_dir()
            device = self.app.audio_frame.get_device()

            all_extracted_folders = []

            if source_option == "skip":
                # Skip extraction, recursively process the folder structure
                if not os.path.isdir(manual_extracted_dir):
                    raise Exception("Manual extracted folder is invalid or doesn't exist.")

                # Recursively find all text files and keep the folder structure
                for root, _, files in os.walk(manual_extracted_dir):
                    if any(file.lower().endswith(".txt") for file in files):  # Process folders with .txt files
                        rel_path = os.path.relpath(root, manual_extracted_dir)
                        output_subfolder = os.path.join(audio_root, rel_path)
                        os.makedirs(output_subfolder, exist_ok=True)
                        all_extracted_folders.append((root, output_subfolder))

                self.update_extract_progress(100)

            elif source_option == "batch":
                if not os.path.isdir(source_folder):
                    raise Exception("Batch source folder is invalid or doesn't exist.")

                # Recursively find all PDFs and process them
                pdf_files = []
                for root, _, files in os.walk(source_folder):
                    for file in files:
                        if file.lower().endswith(".pdf"):
                            pdf_files.append(os.path.join(root, file))

                if not pdf_files:
                    raise Exception("No PDF files found in the selected folder.")

                total_pdfs = len(pdf_files)
                for i, pdf_path in enumerate(pdf_files, start=1):
                    if self.cancellation_flag:
                        raise Exception("Process canceled by user during batch extraction.")

                    # Calculate relative path and replicate folder structure
                    rel_path = os.path.relpath(pdf_path, source_folder)
                    folder_part = os.path.dirname(rel_path)
                    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
                    extracted_text_base = os.path.join(extracted_root, folder_part)
                    os.makedirs(extracted_text_base, exist_ok=True)

                    def extraction_progress_callback(progress):
                        self.update_extract_progress(int((i - 1 + progress / 100) / total_pdfs * 100))
                        if self.cancellation_flag:
                            raise Exception("Process canceled by user.")

                    # Extract PDF to its designated folder
                    extract_book(
                        pdf_path, use_toc=use_toc, extract_mode=extract_mode,
                        output_base_dir=extracted_text_base, progress_callback=extraction_progress_callback
                    )
                    self.log_message(f"Extracted: {pdf_path}, saved to {extracted_text_base}")
                    all_extracted_folders.append((extracted_text_base, os.path.join(audio_root, folder_part)))

                self.update_extract_progress(100)

            else:
                # Single PDF
                pdf_path = self.app.source_frame.get_pdf_path()
                if not os.path.isfile(pdf_path):
                    raise Exception("Single PDF path is invalid or doesn't exist.")
                book_name = os.path.splitext(os.path.basename(pdf_path))[0]

                def extraction_progress_callback(progress):
                    self.update_extract_progress(progress)
                    if self.cancellation_flag:
                        raise Exception("Process canceled by user.")

                extract_book(
                    pdf_path,
                    use_toc=use_toc,
                    extract_mode=extract_mode,
                    output_base_dir=extracted_root,  # e.g. "extracted_pdf"
                    progress_callback=extraction_progress_callback
                )

                self.update_extract_progress(100)
                all_extracted_folders.append((extracted_root, os.path.join(audio_root, book_name)))
                self.log_message(f"Extracted: {book_name}, saved to {extracted_root}")

            # Audiobook generation
            self.set_status("Generating audiobook...")
            self.update_audio_progress(10)

            total_folders = len(all_extracted_folders)
            for i, (input_folder, output_folder) in enumerate(all_extracted_folders, start=1):
                if self.cancellation_flag:
                    raise Exception("Process canceled by user during TTS generation.")

                os.makedirs(output_folder, exist_ok=True)

                generate_audiobooks_kokoro(
                    input_dir=input_folder,
                    model_path=model_path,
                    voicepack_path=voicepack_path,
                    audio_format=audio_format,
                    output_dir=output_folder,
                    max_tokens=chunk_size,
                    device=device,
                    cancellation_flag=lambda: self.cancellation_flag,
                    progress_callback=lambda progress: self.update_audio_progress(int((i - 1 + progress / 100) / total_folders * 100)),
                )

                self.log_message(f"Audiobook generation completed for: {input_folder}. Output: {output_folder}")
                self.update_audio_progress(int(i / total_folders * 100))

            self.set_status("Process completed successfully.")
            self.update_audio_progress(100)

        except Exception as e:
            self.log_message(f"Error occurred: {e}")
            self.set_status("Process failed.")
        finally:
            self.running = False
            self.cancel_button.config(state=tk.DISABLED)
            self.start_button.config(state=tk.NORMAL)


    def _pause_process(self):
        self.pause_event.clear()  # Pause the process
        self.pause_button.config(state=DISABLED)
        self.resume_button.config(state=NORMAL)
        self.log_message("Process paused.")

    def _resume_process(self):
        self.pause_event.set()  # Resume the process
        self.resume_button.config(state=DISABLED)
        self.pause_button.config(state=NORMAL)
        self.log_message("Process resumed.")

    def _cancel_process(self):
        self.log_message("Canceling process...")
        self.cancellation_flag = True
        self.set_status("Process canceling...")

        if self.process_thread and self.process_thread.is_alive():
            self.process_thread.join(timeout=1)  # Wait briefly
            self.running = False
            self.start_button.config(state=NORMAL)
            self.cancel_button.config(state=DISABLED)

    def log_message(self, msg):
        self.log_text.insert(tk.END, msg + "\n")
        self.log_text.see(tk.END)

    def set_status(self, status):
        self.status_text.set(status)

    def update_extract_progress(self, value):
        self.extract_progress.set(value)

    def update_audio_progress(self, value):
        self.audio_progress.set(value)

    def set_estimated_time(self, seconds_left):
        if seconds_left < 0:
            seconds_left = 0
        m, s = divmod(int(seconds_left), 60)
        h, m = divmod(m, 60)
        if h > 0:
            time_str = f"{h}h {m}m {s}s"
        elif m > 0:
            time_str = f"{m}m {s}s"
        else:
            time_str = f"{s}s"
        self.estimated_time_text.set(f"Estimated time remaining: {time_str}")


class AudiobookApp(tb.Window):
    """
    Main application window, uses a ttkbootstrap Notebook with three tabs:
      1) Source (PDF extraction)
      2) Audio (Kokoro model/voicepack selection)
      3) Progress & Logs
    """
    CONFIG_FILE = "config.json"

    def __init__(self, *args, **kwargs):
        self.selected_theme = self._load_theme_from_config()
        super().__init__(*args, themename=self.selected_theme, **kwargs)

        self.title("PDF Narrator (Kokoro Edition)")
        self.geometry("1000x800")

        # Handle window close
        self.protocol("WM_DELETE_WINDOW", self.on_close)

        # Header
        header_frame = tb.Frame(self)
        header_frame.pack(fill=X, pady=10)
        
        title_label = tb.Label(header_frame, text="PDF Narrator", font="-size 16 -weight bold")
        title_label.pack()
        
        subtitle_label = tb.Label(header_frame, text="Convert your PDFs into narrated audiobooks (Kokoro)")
        subtitle_label.pack(pady=(5, 0))

        # Notebook
        self.notebook = tb.Notebook(self)
        self.notebook.pack(fill=BOTH, expand=True, pady=10, padx=10)

        self.source_frame = SourceFrame(self.notebook)
        self.audio_frame = AudioFrame(self.notebook)
        self.progress_frame = ProgressFrame(self.notebook, app=self)

        self.notebook.add(self.source_frame, text="Source")
        self.notebook.add(self.audio_frame, text="Audio")
        self.notebook.add(self.progress_frame, text="Progress & Logs")

        # Footer
        footer_frame = tb.Frame(self)
        footer_frame.pack(fill=X, pady=5)
        
        self.open_output_button = tb.Button(footer_frame, text="Open Extracted Text Folder", command=self._open_output_folder)
        self.open_output_button.pack(side=LEFT, padx=10)

        self.open_audio_output_button = tb.Button(
            footer_frame, 
            text="Open Audiobook Folder", 
            command=self._open_audiobook_folder
        )
        self.open_audio_output_button.pack(side=LEFT, padx=10)

        # Theme Selector
        theme_selector_frame = tb.Frame(footer_frame)
        theme_selector_frame.pack(side=RIGHT, padx=10)

        tb.Label(theme_selector_frame, text="Theme:").pack(side=LEFT)
        self.theme_var = tk.StringVar(value=self.selected_theme)
        themes = tb.Style().theme_names()
        self.theme_combo = tb.Combobox(
            theme_selector_frame, 
            textvariable=self.theme_var, 
            values=themes, 
            state="readonly", 
            width=15
        )
        self.theme_combo.pack(side=LEFT, padx=5)
        self.theme_combo.bind("<<ComboboxSelected>>", self._change_theme)

        exit_button = tb.Button(footer_frame, text="Exit", command=self.on_close)
        exit_button.pack(side=RIGHT, padx=10)

        self.load_config()

    def _open_output_folder(self):
        output_dir = self.source_frame.get_extracted_text_dir()
        if output_dir and os.path.isdir(output_dir):
            if os.name == 'nt':
                os.startfile(output_dir)
            elif os.name == 'posix':
                os.system(f'xdg-open "{output_dir}"')
        else:
            messagebox.showwarning("Warning", "No valid output directory selected.")

    def _open_audiobook_folder(self):
        audio_dir = self.audio_frame.get_audio_output_dir()
        if audio_dir and os.path.isdir(audio_dir):
            if os.name == 'nt':
                os.startfile(audio_dir)
            elif os.name == 'posix':
                os.system(f'xdg-open "{audio_dir}"')
        else:
            messagebox.showwarning("Warning", "No valid audiobook output directory selected.")

    def _change_theme(self, event):
        new_theme = self.theme_var.get()
        tb.Style().theme_use(new_theme)
        self.selected_theme = new_theme

    def _load_theme_from_config(self):
        if os.path.exists(self.CONFIG_FILE):
            try:
                with open(self.CONFIG_FILE, 'r') as f:
                    config = json.load(f)
                    return config.get("theme", "flatly")
            except Exception as e:
                print(f"Failed to load theme from config: {e}")
        return "flatly"  # Default theme
    
    def load_config(self):
        if os.path.exists(self.CONFIG_FILE):
            try:
                with open(self.CONFIG_FILE, 'r') as f:
                    config = json.load(f)

                    # Load source option and paths
                    self.source_frame.source_option.set(config.get("source_option", "single"))
                    self.source_frame.pdf_path.set(config.get("pdf_path", ""))
                    self.source_frame.pdf_folder.set(config.get("pdf_folder", ""))
                    self.source_frame.manual_extracted_dir.set(config.get("manual_extracted_dir", ""))
                    self.source_frame.extracted_text_dir.set(config.get("extracted_text_dir", ""))

                    # Update audio output directory based on source option
                    if config["source_option"] == "single" and config.get("pdf_path"):
                        book_name = os.path.splitext(os.path.basename(config["pdf_path"]))[0]
                    elif config["source_option"] == "batch" and config.get("pdf_folder"):
                        book_name = os.path.basename(config["pdf_folder"].rstrip(os.sep))
                    elif config["source_option"] == "skip" and config.get("manual_extracted_dir"):
                        book_name = os.path.basename(config["manual_extracted_dir"].rstrip(os.sep))
                    else:
                        book_name = ""
                    
                    if book_name:
                        self.update_audio_output_dir(book_name)

                    # Load audio settings
                    self.audio_frame.model_path.set(config.get("model_path", "models/kokoro-v0_19.pth"))
                    self.audio_frame.voicepack_path.set(config.get("voicepack_path", "Kokoro/voices/af_sarah.pt"))
                    self.audio_frame.chunk_size.set(config.get("chunk_size", 510))
                    self.audio_frame.audio_format.set(config.get("audio_format", ".wav"))

                    # Load theme
                    self.selected_theme = config.get("theme", "flatly")
                    self.theme_var.set(self.selected_theme)
                    tb.Style().theme_use(self.selected_theme)

                    # Update UI based on the loaded source option
                    self.source_frame._update_ui()

            except Exception as e:
                print(f"Failed to load config: {e}")


    def save_config(self):
        config = {
            "source_option": self.source_frame.get_source_option(),  # Save selected mode
            "pdf_path": self.source_frame.get_pdf_path(),
            "pdf_folder": self.source_frame.get_pdf_folder(),
            "manual_extracted_dir": self.source_frame.get_manual_extracted_dir(),
            "extracted_text_dir": self.source_frame.get_extracted_text_dir(),  # Save extracted text directory
            "model_path": self.audio_frame.get_model_path(),
            "voicepack_path": self.audio_frame.get_voicepack_path(),
            "chunk_size": self.audio_frame.get_chunk_size(),
            "audio_format": self.audio_frame.get_audio_format(),
            "theme": self.selected_theme,
        }
        try:
            with open(self.CONFIG_FILE, 'w') as f:
                json.dump(config, f, indent=4)
        except Exception as e:
            print(f"Failed to save config: {e}")


    def update_audio_output_dir(self, book_name):
        self.audio_frame.update_audio_output_dir(book_name)

    def on_close(self):
        print("Application is closing.")  # Debug
        self.save_config()
        self.destroy()


#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\config.json
{
  "decoder": {
    "type": "istftnet",
    "upsample_kernel_sizes": [20, 12],
    "upsample_rates": [10, 6],
    "gen_istft_hop_size": 5,
    "gen_istft_n_fft": 20,
    "resblock_dilation_sizes": [
      [1, 3, 5],
      [1, 3, 5],
      [1, 3, 5]
    ],
    "resblock_kernel_sizes": [3, 7, 11],
    "upsample_initial_channel": 512
  },
  "dim_in": 64,
  "dropout": 0.2,
  "hidden_dim": 512,
  "max_conv_dim": 512,
  "max_dur": 50,
  "multispeaker": true,
  "n_layer": 3,
  "n_mels": 80,
  "n_token": 178,
  "style_dim": 128
}

#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\istftnet.py
# https://github.com/yl4579/StyleTTS2/blob/main/Modules/istftnet.py
from scipy.signal import get_window
from torch.nn import Conv1d, ConvTranspose1d
from torch.nn.utils import weight_norm, remove_weight_norm
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# https://github.com/yl4579/StyleTTS2/blob/main/Modules/utils.py
def init_weights(m, mean=0.0, std=0.01):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        m.weight.data.normal_(mean, std)

def get_padding(kernel_size, dilation=1):
    return int((kernel_size*dilation - dilation)/2)

LRELU_SLOPE = 0.1

class AdaIN1d(nn.Module):
    def __init__(self, style_dim, num_features):
        super().__init__()
        self.norm = nn.InstanceNorm1d(num_features, affine=False)
        self.fc = nn.Linear(style_dim, num_features*2)

    def forward(self, x, s):
        h = self.fc(s)
        h = h.view(h.size(0), h.size(1), 1)
        gamma, beta = torch.chunk(h, chunks=2, dim=1)
        return (1 + gamma) * self.norm(x) + beta

class AdaINResBlock1(torch.nn.Module):
    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5), style_dim=64):
        super(AdaINResBlock1, self).__init__()
        self.convs1 = nn.ModuleList([
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],
                               padding=get_padding(kernel_size, dilation[0]))),
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],
                               padding=get_padding(kernel_size, dilation[1]))),
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],
                               padding=get_padding(kernel_size, dilation[2])))
        ])
        self.convs1.apply(init_weights)

        self.convs2 = nn.ModuleList([
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,
                               padding=get_padding(kernel_size, 1))),
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,
                               padding=get_padding(kernel_size, 1))),
            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,
                               padding=get_padding(kernel_size, 1)))
        ])
        self.convs2.apply(init_weights)
        
        self.adain1 = nn.ModuleList([
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
        ])
        
        self.adain2 = nn.ModuleList([
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
        ])
        
        self.alpha1 = nn.ParameterList([nn.Parameter(torch.ones(1, channels, 1)) for i in range(len(self.convs1))])
        self.alpha2 = nn.ParameterList([nn.Parameter(torch.ones(1, channels, 1)) for i in range(len(self.convs2))])


    def forward(self, x, s):
        for c1, c2, n1, n2, a1, a2 in zip(self.convs1, self.convs2, self.adain1, self.adain2, self.alpha1, self.alpha2):
            xt = n1(x, s)
            xt = xt + (1 / a1) * (torch.sin(a1 * xt) ** 2)  # Snake1D
            xt = c1(xt)
            xt = n2(xt, s)
            xt = xt + (1 / a2) * (torch.sin(a2 * xt) ** 2)  # Snake1D
            xt = c2(xt)
            x = xt + x
        return x

    def remove_weight_norm(self):
        for l in self.convs1:
            remove_weight_norm(l)
        for l in self.convs2:
            remove_weight_norm(l)
            
class TorchSTFT(torch.nn.Module):
    def __init__(self, filter_length=800, hop_length=200, win_length=800, window='hann'):
        super().__init__()
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        self.window = torch.from_numpy(get_window(window, win_length, fftbins=True).astype(np.float32))

    def transform(self, input_data):
        forward_transform = torch.stft(
            input_data,
            self.filter_length, self.hop_length, self.win_length, window=self.window.to(input_data.device),
            return_complex=True)

        return torch.abs(forward_transform), torch.angle(forward_transform)

    def inverse(self, magnitude, phase):
        inverse_transform = torch.istft(
            magnitude * torch.exp(phase * 1j),
            self.filter_length, self.hop_length, self.win_length, window=self.window.to(magnitude.device))

        return inverse_transform.unsqueeze(-2)  # unsqueeze to stay consistent with conv_transpose1d implementation

    def forward(self, input_data):
        self.magnitude, self.phase = self.transform(input_data)
        reconstruction = self.inverse(self.magnitude, self.phase)
        return reconstruction
    
class SineGen(torch.nn.Module):
    """ Definition of sine generator
    SineGen(samp_rate, harmonic_num = 0,
            sine_amp = 0.1, noise_std = 0.003,
            voiced_threshold = 0,
            flag_for_pulse=False)
    samp_rate: sampling rate in Hz
    harmonic_num: number of harmonic overtones (default 0)
    sine_amp: amplitude of sine-wavefrom (default 0.1)
    noise_std: std of Gaussian noise (default 0.003)
    voiced_thoreshold: F0 threshold for U/V classification (default 0)
    flag_for_pulse: this SinGen is used inside PulseGen (default False)
    Note: when flag_for_pulse is True, the first time step of a voiced
        segment is always sin(np.pi) or cos(0)
    """

    def __init__(self, samp_rate, upsample_scale, harmonic_num=0,
                 sine_amp=0.1, noise_std=0.003,
                 voiced_threshold=0,
                 flag_for_pulse=False):
        super(SineGen, self).__init__()
        self.sine_amp = sine_amp
        self.noise_std = noise_std
        self.harmonic_num = harmonic_num
        self.dim = self.harmonic_num + 1
        self.sampling_rate = samp_rate
        self.voiced_threshold = voiced_threshold
        self.flag_for_pulse = flag_for_pulse
        self.upsample_scale = upsample_scale

    def _f02uv(self, f0):
        # generate uv signal
        uv = (f0 > self.voiced_threshold).type(torch.float32)
        return uv

    def _f02sine(self, f0_values):
        """ f0_values: (batchsize, length, dim)
            where dim indicates fundamental tone and overtones
        """
        # convert to F0 in rad. The interger part n can be ignored
        # because 2 * np.pi * n doesn't affect phase
        rad_values = (f0_values / self.sampling_rate) % 1

        # initial phase noise (no noise for fundamental component)
        rand_ini = torch.rand(f0_values.shape[0], f0_values.shape[2], \
                              device=f0_values.device)
        rand_ini[:, 0] = 0
        rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini

        # instantanouse phase sine[t] = sin(2*pi \sum_i=1 ^{t} rad)
        if not self.flag_for_pulse:
#             # for normal case

#             # To prevent torch.cumsum numerical overflow,
#             # it is necessary to add -1 whenever \sum_k=1^n rad_value_k > 1.
#             # Buffer tmp_over_one_idx indicates the time step to add -1.
#             # This will not change F0 of sine because (x-1) * 2*pi = x * 2*pi
#             tmp_over_one = torch.cumsum(rad_values, 1) % 1
#             tmp_over_one_idx = (padDiff(tmp_over_one)) < 0
#             cumsum_shift = torch.zeros_like(rad_values)
#             cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0

#             phase = torch.cumsum(rad_values, dim=1) * 2 * np.pi
            rad_values = torch.nn.functional.interpolate(rad_values.transpose(1, 2), 
                                                         scale_factor=1/self.upsample_scale, 
                                                         mode="linear").transpose(1, 2)
    
#             tmp_over_one = torch.cumsum(rad_values, 1) % 1
#             tmp_over_one_idx = (padDiff(tmp_over_one)) < 0
#             cumsum_shift = torch.zeros_like(rad_values)
#             cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0
    
            phase = torch.cumsum(rad_values, dim=1) * 2 * np.pi
            phase = torch.nn.functional.interpolate(phase.transpose(1, 2) * self.upsample_scale, 
                                                    scale_factor=self.upsample_scale, mode="linear").transpose(1, 2)
            sines = torch.sin(phase)
            
        else:
            # If necessary, make sure that the first time step of every
            # voiced segments is sin(pi) or cos(0)
            # This is used for pulse-train generation

            # identify the last time step in unvoiced segments
            uv = self._f02uv(f0_values)
            uv_1 = torch.roll(uv, shifts=-1, dims=1)
            uv_1[:, -1, :] = 1
            u_loc = (uv < 1) * (uv_1 > 0)

            # get the instantanouse phase
            tmp_cumsum = torch.cumsum(rad_values, dim=1)
            # different batch needs to be processed differently
            for idx in range(f0_values.shape[0]):
                temp_sum = tmp_cumsum[idx, u_loc[idx, :, 0], :]
                temp_sum[1:, :] = temp_sum[1:, :] - temp_sum[0:-1, :]
                # stores the accumulation of i.phase within
                # each voiced segments
                tmp_cumsum[idx, :, :] = 0
                tmp_cumsum[idx, u_loc[idx, :, 0], :] = temp_sum

            # rad_values - tmp_cumsum: remove the accumulation of i.phase
            # within the previous voiced segment.
            i_phase = torch.cumsum(rad_values - tmp_cumsum, dim=1)

            # get the sines
            sines = torch.cos(i_phase * 2 * np.pi)
        return sines

    def forward(self, f0):
        """ sine_tensor, uv = forward(f0)
        input F0: tensor(batchsize=1, length, dim=1)
                  f0 for unvoiced steps should be 0
        output sine_tensor: tensor(batchsize=1, length, dim)
        output uv: tensor(batchsize=1, length, 1)
        """
        f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim,
                             device=f0.device)
        # fundamental component
        fn = torch.multiply(f0, torch.FloatTensor([[range(1, self.harmonic_num + 2)]]).to(f0.device))

        # generate sine waveforms
        sine_waves = self._f02sine(fn) * self.sine_amp

        # generate uv signal
        # uv = torch.ones(f0.shape)
        # uv = uv * (f0 > self.voiced_threshold)
        uv = self._f02uv(f0)

        # noise: for unvoiced should be similar to sine_amp
        #        std = self.sine_amp/3 -> max value ~ self.sine_amp
        # .       for voiced regions is self.noise_std
        noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3
        noise = noise_amp * torch.randn_like(sine_waves)

        # first: set the unvoiced part to 0 by uv
        # then: additive noise
        sine_waves = sine_waves * uv + noise
        return sine_waves, uv, noise


class SourceModuleHnNSF(torch.nn.Module):
    """ SourceModule for hn-nsf
    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0)
    sampling_rate: sampling_rate in Hz
    harmonic_num: number of harmonic above F0 (default: 0)
    sine_amp: amplitude of sine source signal (default: 0.1)
    add_noise_std: std of additive Gaussian noise (default: 0.003)
        note that amplitude of noise in unvoiced is decided
        by sine_amp
    voiced_threshold: threhold to set U/V given F0 (default: 0)
    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
    F0_sampled (batchsize, length, 1)
    Sine_source (batchsize, length, 1)
    noise_source (batchsize, length 1)
    uv (batchsize, length, 1)
    """

    def __init__(self, sampling_rate, upsample_scale, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0):
        super(SourceModuleHnNSF, self).__init__()

        self.sine_amp = sine_amp
        self.noise_std = add_noise_std

        # to produce sine waveforms
        self.l_sin_gen = SineGen(sampling_rate, upsample_scale, harmonic_num,
                                 sine_amp, add_noise_std, voiced_threshod)

        # to merge source harmonics into a single excitation
        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)
        self.l_tanh = torch.nn.Tanh()

    def forward(self, x):
        """
        Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
        F0_sampled (batchsize, length, 1)
        Sine_source (batchsize, length, 1)
        noise_source (batchsize, length 1)
        """
        # source for harmonic branch
        with torch.no_grad():
            sine_wavs, uv, _ = self.l_sin_gen(x)
        sine_merge = self.l_tanh(self.l_linear(sine_wavs))

        # source for noise branch, in the same shape as uv
        noise = torch.randn_like(uv) * self.sine_amp / 3
        return sine_merge, noise, uv
def padDiff(x):
    return F.pad(F.pad(x, (0,0,-1,1), 'constant', 0) - x, (0,0,0,-1), 'constant', 0)

    
class Generator(torch.nn.Module):
    def __init__(self, style_dim, resblock_kernel_sizes, upsample_rates, upsample_initial_channel, resblock_dilation_sizes, upsample_kernel_sizes, gen_istft_n_fft, gen_istft_hop_size):
        super(Generator, self).__init__()

        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_rates)
        resblock = AdaINResBlock1

        self.m_source = SourceModuleHnNSF(
                    sampling_rate=24000,
                    upsample_scale=np.prod(upsample_rates) * gen_istft_hop_size,
                    harmonic_num=8, voiced_threshod=10)
        self.f0_upsamp = torch.nn.Upsample(scale_factor=np.prod(upsample_rates) * gen_istft_hop_size)
        self.noise_convs = nn.ModuleList()
        self.noise_res = nn.ModuleList()
        
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(weight_norm(
                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),
                                k, u, padding=(k-u)//2)))

        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = upsample_initial_channel//(2**(i+1))
            for j, (k, d) in enumerate(zip(resblock_kernel_sizes,resblock_dilation_sizes)):
                self.resblocks.append(resblock(ch, k, d, style_dim))
                
            c_cur = upsample_initial_channel // (2 ** (i + 1))
            
            if i + 1 < len(upsample_rates):  #
                stride_f0 = np.prod(upsample_rates[i + 1:])
                self.noise_convs.append(Conv1d(
                    gen_istft_n_fft + 2, c_cur, kernel_size=stride_f0 * 2, stride=stride_f0, padding=(stride_f0+1) // 2))
                self.noise_res.append(resblock(c_cur, 7, [1,3,5], style_dim))
            else:
                self.noise_convs.append(Conv1d(gen_istft_n_fft + 2, c_cur, kernel_size=1))
                self.noise_res.append(resblock(c_cur, 11, [1,3,5], style_dim))
                
                
        self.post_n_fft = gen_istft_n_fft
        self.conv_post = weight_norm(Conv1d(ch, self.post_n_fft + 2, 7, 1, padding=3))
        self.ups.apply(init_weights)
        self.conv_post.apply(init_weights)
        self.reflection_pad = torch.nn.ReflectionPad1d((1, 0))
        self.stft = TorchSTFT(filter_length=gen_istft_n_fft, hop_length=gen_istft_hop_size, win_length=gen_istft_n_fft)
        
        
    def forward(self, x, s, f0):
        with torch.no_grad():
            f0 = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t

            har_source, noi_source, uv = self.m_source(f0)
            har_source = har_source.transpose(1, 2).squeeze(1)
            har_spec, har_phase = self.stft.transform(har_source)
            har = torch.cat([har_spec, har_phase], dim=1)
        
        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, LRELU_SLOPE)
            x_source = self.noise_convs[i](har)
            x_source = self.noise_res[i](x_source, s)

            x = self.ups[i](x)
            if i == self.num_upsamples - 1:
                x = self.reflection_pad(x)

            x = x + x_source
            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i*self.num_kernels+j](x, s)
                else:
                    xs += self.resblocks[i*self.num_kernels+j](x, s)
            x = xs / self.num_kernels
        x = F.leaky_relu(x)
        x = self.conv_post(x)
        spec = torch.exp(x[:,:self.post_n_fft // 2 + 1, :])
        phase = torch.sin(x[:, self.post_n_fft // 2 + 1:, :])
        return self.stft.inverse(spec, phase)
    
    def fw_phase(self, x, s):
        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, LRELU_SLOPE)
            x = self.ups[i](x)
            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i*self.num_kernels+j](x, s)
                else:
                    xs += self.resblocks[i*self.num_kernels+j](x, s)
            x = xs / self.num_kernels
        x = F.leaky_relu(x)
        x = self.reflection_pad(x)
        x = self.conv_post(x)
        spec = torch.exp(x[:,:self.post_n_fft // 2 + 1, :])
        phase = torch.sin(x[:, self.post_n_fft // 2 + 1:, :])
        return spec, phase

    def remove_weight_norm(self):
        print('Removing weight norm...')
        for l in self.ups:
            remove_weight_norm(l)
        for l in self.resblocks:
            l.remove_weight_norm()
        remove_weight_norm(self.conv_pre)
        remove_weight_norm(self.conv_post)

        
class AdainResBlk1d(nn.Module):
    def __init__(self, dim_in, dim_out, style_dim=64, actv=nn.LeakyReLU(0.2),
                 upsample='none', dropout_p=0.0):
        super().__init__()
        self.actv = actv
        self.upsample_type = upsample
        self.upsample = UpSample1d(upsample)
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out, style_dim)
        self.dropout = nn.Dropout(dropout_p)
        
        if upsample == 'none':
            self.pool = nn.Identity()
        else:
            self.pool = weight_norm(nn.ConvTranspose1d(dim_in, dim_in, kernel_size=3, stride=2, groups=dim_in, padding=1, output_padding=1))
        
        
    def _build_weights(self, dim_in, dim_out, style_dim):
        self.conv1 = weight_norm(nn.Conv1d(dim_in, dim_out, 3, 1, 1))
        self.conv2 = weight_norm(nn.Conv1d(dim_out, dim_out, 3, 1, 1))
        self.norm1 = AdaIN1d(style_dim, dim_in)
        self.norm2 = AdaIN1d(style_dim, dim_out)
        if self.learned_sc:
            self.conv1x1 = weight_norm(nn.Conv1d(dim_in, dim_out, 1, 1, 0, bias=False))

    def _shortcut(self, x):
        x = self.upsample(x)
        if self.learned_sc:
            x = self.conv1x1(x)
        return x

    def _residual(self, x, s):
        x = self.norm1(x, s)
        x = self.actv(x)
        x = self.pool(x)
        x = self.conv1(self.dropout(x))
        x = self.norm2(x, s)
        x = self.actv(x)
        x = self.conv2(self.dropout(x))
        return x

    def forward(self, x, s):
        out = self._residual(x, s)
        out = (out + self._shortcut(x)) / np.sqrt(2)
        return out
    
class UpSample1d(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type

    def forward(self, x):
        if self.layer_type == 'none':
            return x
        else:
            return F.interpolate(x, scale_factor=2, mode='nearest')

class Decoder(nn.Module):
    def __init__(self, dim_in=512, F0_channel=512, style_dim=64, dim_out=80, 
                resblock_kernel_sizes = [3,7,11],
                upsample_rates = [10, 6],
                upsample_initial_channel=512,
                resblock_dilation_sizes=[[1,3,5], [1,3,5], [1,3,5]],
                upsample_kernel_sizes=[20, 12], 
                gen_istft_n_fft=20, gen_istft_hop_size=5):
        super().__init__()
        
        self.decode = nn.ModuleList()
        
        self.encode = AdainResBlk1d(dim_in + 2, 1024, style_dim)
        
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 512, style_dim, upsample=True))

        self.F0_conv = weight_norm(nn.Conv1d(1, 1, kernel_size=3, stride=2, groups=1, padding=1))
        
        self.N_conv = weight_norm(nn.Conv1d(1, 1, kernel_size=3, stride=2, groups=1, padding=1))
        
        self.asr_res = nn.Sequential(
            weight_norm(nn.Conv1d(512, 64, kernel_size=1)),
        )
        
        
        self.generator = Generator(style_dim, resblock_kernel_sizes, upsample_rates, 
                                   upsample_initial_channel, resblock_dilation_sizes, 
                                   upsample_kernel_sizes, gen_istft_n_fft, gen_istft_hop_size)
        
    def forward(self, asr, F0_curve, N, s):
        F0 = self.F0_conv(F0_curve.unsqueeze(1))
        N = self.N_conv(N.unsqueeze(1))
        
        x = torch.cat([asr, F0, N], axis=1)
        x = self.encode(x, s)
        
        asr_res = self.asr_res(asr)
        
        res = True
        for block in self.decode:
            if res:
                x = torch.cat([x, asr_res, F0, N], axis=1)
            x = block(x, s)
            if block.upsample_type != "none":
                res = False
                
        x = self.generator(x, s, F0_curve)
        return x


#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\kokoro.py
## kokoro.py
import phonemizer
import re
import torch
import time

def split_num(num):
    num = num.group()
    if '.' in num:
        return num
    elif ':' in num:
        h, m = [int(n) for n in num.split(':')]
        if m == 0:
            return f"{h} o'clock"
        elif m < 10:
            return f'{h} oh {m}'
        return f'{h} {m}'
    year = int(num[:4])
    if year < 1100 or year % 1000 < 10:
        return num
    left, right = num[:2], int(num[2:4])
    s = 's' if num.endswith('s') else ''
    if 100 <= year % 1000 <= 999:
        if right == 0:
            return f'{left} hundred{s}'
        elif right < 10:
            return f'{left} oh {right}{s}'
    return f'{left} {right}{s}'

def flip_money(m):
    m = m.group()
    bill = 'dollar' if m[0] == '$' else 'pound'
    if m[-1].isalpha():
        return f'{m[1:]} {bill}s'
    elif '.' not in m:
        s = '' if m[1:] == '1' else 's'
        return f'{m[1:]} {bill}{s}'
    b, c = m[1:].split('.')
    s = '' if b == '1' else 's'
    c = int(c.ljust(2, '0'))
    coins = f"cent{'' if c == 1 else 's'}" if m[0] == '$' else ('penny' if c == 1 else 'pence')
    return f'{b} {bill}{s} and {c} {coins}'

def point_num(num):
    a, b = num.group().split('.')
    return ' point '.join([a, ' '.join(b)])

def normalize_text(text):
    text = text.replace(chr(8216), "'").replace(chr(8217), "'")  # Replace curly apostrophes with straight apostrophes
    text = text.replace('Â«', chr(8220)).replace('Â»', chr(8221))  # Replace guillemets with curly quotes
    text = text.replace(chr(8220), '"').replace(chr(8221), '"')  # Normalize curly quotes to straight quotes
    text = text.replace('(', ',').replace(')', ',')  # Replace "(" and ")" with commas
    text = text.replace(';', '.')  # Replace ";" with "."
    text = text.replace('â€”', ' ,')  # Replace long dash "â€”" with " ,"
    text = text.replace('!', '.')  # Replace "!" with "."
    for a, b in zip('ã€�ã€‚ï¼�ï¼Œï¼šï¼›ï¼Ÿ', ',.!,:;?'):  # Replace non-standard punctuation with standard equivalents
        text = text.replace(a, b + ' ')
    text = re.sub(r'[^\S \n]', ' ', text)  # Replace all non-visible characters (except spaces and newlines) with a space
    text = re.sub(r'  +', ' ', text)  # Collapse multiple spaces into a single space
    text = re.sub(r'(?<=\n) +(?=\n)', '', text)  # Remove spaces on empty lines
    text = re.sub(r'\bD[Rr]\.(?= [A-Z])', 'Doctor', text)  # Replace "Dr." or "dr." with "Doctor" if followed by a capitalized word
    text = re.sub(r'\b(?:Mr\.|MR\.(?= [A-Z]))', 'Mister', text)  # Replace "Mr." or "MR." with "Mister" if followed by a capitalized word
    text = re.sub(r'\b(?:Ms\.|MS\.(?= [A-Z]))', 'Miss', text)  # Replace "Ms." or "MS." with "Miss" if followed by a capitalized word
    text = re.sub(r'\b(?:Mrs\.|MRS\.(?= [A-Z]))', 'Mrs', text)  # Replace "Mrs." or "MRS." with "Mrs" if followed by a capitalized word
    text = re.sub(r'\betc\.(?! [A-Z])', 'etc', text)  # Replace "etc." with "etc" if not followed by a capitalized word
    text = re.sub(r'(?i)\b(y)eah?\b', r"\1e'a", text)  # Replace "yeah" or "yea" (case-insensitive) with "ye'a"
    text = re.sub(
        r'\d*\.\d+|\b\d{4}s?\b|(?<!:)\b(?:[1-9]|1[0-2]):[0-5]\d\b(?!:)',
        split_num,  # Match and process numbers, years, and times
        text
    )
    text = re.sub(r'(?<=\d),(?=\d)', '', text)  # Remove commas inside numbers (e.g., "1,000" -> "1000")
    text = re.sub(
        r'(?i)[$Â£]\d+(?:\.\d+)?(?: hundred| thousand| (?:[bm]|tr)illion)*\b|[$Â£]\d+\.\d\d?\b',
        flip_money,  # Process monetary amounts (e.g., "$10.99")
        text
    )
    text = re.sub(r'\d*\.\d+', point_num, text)  # Process decimal numbers
    text = re.sub(r'(?<=\d)-(?=\d)', ' to ', text)  # Replace hyphens between numbers with " to "
    text = re.sub(r'(?<=\d)S', ' S', text)  # Add a space before "S" following a number
    text = re.sub(r"(?<=[BCDFGHJ-NP-TV-Z])'?s\b", "'S", text)  # Normalize possessive "'s" after uppercase letters
    text = re.sub(r"(?<=X')S\b", 's', text)  # Lowercase "S" following "X'" (special case)
    text = re.sub(r'(?:[A-Za-z]\.){2,} [a-z]', lambda m: m.group().replace('.', '-'), text)  # Replace repeated initials (e.g., "U.S.") with hyphens (e.g., "U-S")
    text = re.sub(r'(?i)(?<=[A-Z])\.(?=[A-Z])', '-', text)  # Replace periods between uppercase letters with hyphens (e.g., "A.B." -> "A-B")
    return text.strip()

def chunk_text(text, lang, max_tokens=510):
    # Phonemize and tokenize the entire text
    phonemized_text = phonemize(text, lang)
    tokenized_text = tokenize(phonemized_text)

    chunks = []
    start = 0

    while start < len(tokenized_text):
        end = min(start + max_tokens, len(tokenized_text))

        # Look for the nearest punctuation in the current range (prefer closer to `end`)
        best_punctuation_pos = -1
        for pos in range(start, end):
            if phonemized_text[pos] in ".,!?":  # Consider valid punctuation marks
                best_punctuation_pos = pos
                #print(f"Found punctuation at position {pos}: {phonemized_text[pos]}")
        # If we found a punctuation, use it as the chunk's end
        if best_punctuation_pos != -1:
            chunk_end = best_punctuation_pos + 1  # Include the punctuation in the chunk
            #print(f"Using punctuation as chunk end: {chunk_end}")
        else:
            chunk_end = end  # No punctuation found, use the `max_tokens` range
            #print(f"Using max_tokens as chunk end: {chunk_end}")

        # Append the chunk
        chunks.append((
            phonemized_text[start:chunk_end],
            tokenized_text[start:chunk_end]
        ))

        # Move start to the end of the current chunk
        start = chunk_end

    return chunks

def tokens_to_text(tokens):
    """
    Reconstruct text from a list of token IDs using the global VOCAB.
    """
    reverse_dict = {v: k for k, v in VOCAB.items()}
    return ''.join(reverse_dict[t] for t in tokens if t in reverse_dict)

def chunk_text_by_lines(
    text,
    lang,
    max_tokens=510,
    cancellation_flag=None
):
    """
    1) Split the original text by lines (unphonemized).
    2) For each line, phonemize + tokenize.
       - If the line itself has more tokens than max_tokens, break it into forced
         sub-chunks of up to max_tokens.
       - Otherwise, try to accumulate this line into the current chunk if it fits.
         If it doesn't fit, finalize the current chunk and start a new one.
    3) If cancellation_flag is True at any time, we stop generating more chunks immediately.
    4) Return a list of (chunk_text, chunk_tokens) tuples, each guaranteed â‰¤ max_tokens.
    """

    lines = text.split('\n')
    chunks = []
    current_chunk_tokens = []
    current_chunk_size = 0  # Number of tokens in the current chunk

    for raw_line in lines:
        # Check cancellation before processing each line
        if cancellation_flag and cancellation_flag():
            print("Stopping chunk_text_by_lines due to cancellation_flag.")
            break

        line = raw_line.strip()
        if not line:
            continue  # Skip empty lines

        # 1) Phonemize and tokenize this line
        phonemized_line = phonemize(line, lang)
        line_tokens = tokenize(phonemized_line)

        # 2) If the line itself exceeds max_tokens, break it into forced sub-chunks
        #    before trying to accumulate it in current_chunk_tokens.
        idx = 0
        while idx < len(line_tokens):
            # Check cancellation inside the loop as well
            if cancellation_flag and cancellation_flag():
                print("Stopping chunk_text_by_lines due to cancellation_flag.")
                break

            remaining = len(line_tokens) - idx
            # Sub-chunk = either the whole remainder if it fits in max_tokens,
            # or exactly max_tokens if it's bigger
            sub_chunk_size = min(remaining, max_tokens)

            sub_chunk = line_tokens[idx : idx + sub_chunk_size]
            idx += sub_chunk_size

            # If sub-chunk is the entire (or part of) a large line, we must see if we
            # can accumulate it in the *current* chunk or if we need to finalize first.

            # If sub-chunk alone is bigger than max_tokens (only possible if we forced
            # sub-chunk_size = max_tokens, weâ€™ll still handle partial line in multiple loops).
            # But let's check if we can *accumulate* sub_chunk on top of current_chunk.
            if current_chunk_size + len(sub_chunk) <= max_tokens:
                # We can fit the sub-chunk into the current chunk
                current_chunk_tokens.extend(sub_chunk)
                current_chunk_size += len(sub_chunk)
            else:
                # We cannot fit sub-chunk in the current chunk â†’ finalize the current chunk
                if current_chunk_tokens:
                    chunk_text_reconstructed = tokens_to_text(current_chunk_tokens)
                    chunks.append((chunk_text_reconstructed, current_chunk_tokens))

                # Start a new chunk with the sub-chunk
                current_chunk_tokens = sub_chunk
                current_chunk_size = len(sub_chunk)

            # If the sub-chunk exactly fills the chunk, finalize immediately
            if current_chunk_size == max_tokens:
                chunk_text_reconstructed = tokens_to_text(current_chunk_tokens)
                chunks.append((chunk_text_reconstructed, current_chunk_tokens))
                current_chunk_tokens = []
                current_chunk_size = 0

        # If canceled mid-line chunking, break out of the outer loop
        if cancellation_flag and cancellation_flag():
            print("Stopping chunk_text_by_lines after partial line due to cancellation_flag.")
            break

    # 3) Finalize any leftover tokens in the current chunk if not empty
    if current_chunk_tokens and (not cancellation_flag or not cancellation_flag()):
        chunk_text_reconstructed = tokens_to_text(current_chunk_tokens)
        chunks.append((chunk_text_reconstructed, current_chunk_tokens))

    return chunks


def get_vocab():
    _pad = "$"
    _punctuation = ';:,.!?Â¡Â¿â€”â€¦"Â«Â»â€œâ€� '
    _letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
    _letters_ipa = "É‘É�É’Ã¦É“Ê™Î²É”É•Ã§É—É–Ã°Ê¤É™É˜ÉšÉ›ÉœÉ�ÉžÉŸÊ„É¡É É¢Ê›É¦É§Ä§É¥ÊœÉ¨ÉªÊ�É­É¬É«É®ÊŸÉ±É¯É°Å‹É³É²É´Ã¸ÉµÉ¸Î¸Å“É¶Ê˜É¹ÉºÉ¾É»Ê€Ê�É½Ê‚ÊƒÊˆÊ§Ê‰ÊŠÊ‹â±±ÊŒÉ£É¤Ê�Ï‡ÊŽÊ�Ê‘Ê�Ê’Ê”Ê¡Ê•Ê¢Ç€Ç�Ç‚ÇƒËˆËŒË�Ë‘Ê¼Ê´Ê°Ê±Ê²Ê·Ë Ë¤Ëžâ†“â†‘â†’â†—â†˜'Ì©'áµ»"
    symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
    dicts = {}
    for i in range(len((symbols))):
        dicts[symbols[i]] = i
    return dicts

VOCAB = get_vocab()
def tokenize(ps):
    return [i for i in map(VOCAB.get, ps) if i is not None]

phonemizers = dict(
    a=phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True, punctuation_marks=".,!?", with_stress=True),
    b=phonemizer.backend.EspeakBackend(language='en-gb', preserve_punctuation=True, punctuation_marks=".,!?", with_stress=True),
)
def phonemize(text, lang, norm=True):
    if norm:
        text = normalize_text(text)
    ps = phonemizers[lang].phonemize([text])
    ps = ps[0] if ps else ''
    
    
    ps = ps.replace('kÉ™kËˆoË�É¹oÊŠ', 'kËˆoÊŠkÉ™É¹oÊŠ').replace('kÉ™kËˆÉ”Ë�É¹É™ÊŠ', 'kËˆÉ™ÊŠkÉ™É¹É™ÊŠ')
    ps = ps.replace('Ê²', 'j').replace('r', 'É¹').replace('x', 'k').replace('É¬', 'l')
    ps = re.sub(r'(?<=[a-zÉ¹Ë�])(?=hËˆÊŒndÉ¹Éªd)', ' ', ps)
    ps = re.sub(r' z(?=[;:,.!?Â¡Â¿â€”â€¦"Â«Â»â€œâ€� ]|$)', 'z', ps)
    if lang == 'a':
        ps = re.sub(r'(?<=nËˆaÉªn)ti(?!Ë�)', 'di', ps)
    ps = ''.join(filter(lambda p: p in VOCAB, ps))
    return ps.strip()


def length_to_mask(lengths):
    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)
    mask = torch.gt(mask+1, lengths.unsqueeze(1))
    return mask

@torch.no_grad()
def forward(model, tokens, ref_s, speed):
    device = ref_s.device
    tokens = torch.LongTensor([[0, *tokens, 0]]).to(device)
    input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)
    text_mask = length_to_mask(input_lengths).to(device)
    bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())
    d_en = model.bert_encoder(bert_dur).transpose(-1, -2)
    s = ref_s[:, 128:]
    d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)
    x, _ = model.predictor.lstm(d)
    duration = model.predictor.duration_proj(x)
    duration = torch.sigmoid(duration).sum(axis=-1) / speed
    pred_dur = torch.round(duration).clamp(min=1).long()
    pred_aln_trg = torch.zeros(input_lengths, pred_dur.sum().item())
    c_frame = 0
    for i in range(pred_aln_trg.size(0)):
        pred_aln_trg[i, c_frame:c_frame + pred_dur[0,i].item()] = 1
        c_frame += pred_dur[0,i].item()
    en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)
    F0_pred, N_pred = model.predictor.F0Ntrain(en, s)
    t_en = model.text_encoder(tokens, input_lengths, text_mask)
    asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)
    return model.decoder(asr, F0_pred, N_pred, ref_s[:, :128]).squeeze().cpu().numpy()


def generate(
    model,
    text,
    voicepack,
    lang='a',
    speed=1,
    max_tokens=510,
    progress_callback=None,
    cancellation_flag= None
):
    """
    Generate audio from `text` using the provided Kokoro model & voicepack.
    Automatically splits text into token chunks (via `chunk_text_by_lines`).
    Calls `progress_callback(chars_in_chunk, chunk_duration)` after each chunk if provided.
    """
    # 1. Split text into chunks by lines & tokens
    chunks = chunk_text_by_lines(text, lang, max_tokens=max_tokens, cancellation_flag=cancellation_flag)
    audio_output = []
    phonemes_output = []


    # 2. For each chunk, run the TTS
    for i, (chunk_text, chunk_tokens) in enumerate(chunks, 1):
        # Check cancellation right before processing each chunk
        if cancellation_flag and cancellation_flag():
            print("Cancelled before chunk", i)
            break

        print(f"Processing chunk {i} with {len(chunk_tokens)} tokens")
        chunk_start_time = time.time()

        ref_s = voicepack[len(chunk_tokens)]
        out = forward(model, chunk_tokens, ref_s, speed)

        chunk_end_time = time.time()
        chunk_duration = chunk_end_time - chunk_start_time

        if progress_callback:
            progress_callback(len(chunk_text), chunk_duration)

        phonemes_output.append(chunk_text)
        audio_output.append(out)

    return audio_output, phonemes_output




#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\models.py
# https://github.com/yl4579/StyleTTS2/blob/main/models.py
from Kokoro.istftnet import Decoder
from munch import Munch
from pathlib import Path
from Kokoro.plbert import load_plbert
from torch.nn.utils import weight_norm, spectral_norm
import json
import numpy as np
import os
import os.path as osp
import torch
import torch.nn as nn
import torch.nn.functional as F

class LearnedDownSample(nn.Module):
    def __init__(self, layer_type, dim_in):
        super().__init__()
        self.layer_type = layer_type

        if self.layer_type == 'none':
            self.conv = nn.Identity()
        elif self.layer_type == 'timepreserve':
            self.conv = spectral_norm(nn.Conv2d(dim_in, dim_in, kernel_size=(3, 1), stride=(2, 1), groups=dim_in, padding=(1, 0)))
        elif self.layer_type == 'half':
            self.conv = spectral_norm(nn.Conv2d(dim_in, dim_in, kernel_size=(3, 3), stride=(2, 2), groups=dim_in, padding=1))
        else:
            raise RuntimeError('Got unexpected donwsampletype %s, expected is [none, timepreserve, half]' % self.layer_type)
            
    def forward(self, x):
        return self.conv(x)

class LearnedUpSample(nn.Module):
    def __init__(self, layer_type, dim_in):
        super().__init__()
        self.layer_type = layer_type
        
        if self.layer_type == 'none':
            self.conv = nn.Identity()
        elif self.layer_type == 'timepreserve':
            self.conv = nn.ConvTranspose2d(dim_in, dim_in, kernel_size=(3, 1), stride=(2, 1), groups=dim_in, output_padding=(1, 0), padding=(1, 0))
        elif self.layer_type == 'half':
            self.conv = nn.ConvTranspose2d(dim_in, dim_in, kernel_size=(3, 3), stride=(2, 2), groups=dim_in, output_padding=1, padding=1)
        else:
            raise RuntimeError('Got unexpected upsampletype %s, expected is [none, timepreserve, half]' % self.layer_type)


    def forward(self, x):
        return self.conv(x)

class DownSample(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type

    def forward(self, x):
        if self.layer_type == 'none':
            return x
        elif self.layer_type == 'timepreserve':
            return F.avg_pool2d(x, (2, 1))
        elif self.layer_type == 'half':
            if x.shape[-1] % 2 != 0:
                x = torch.cat([x, x[..., -1].unsqueeze(-1)], dim=-1)
            return F.avg_pool2d(x, 2)
        else:
            raise RuntimeError('Got unexpected donwsampletype %s, expected is [none, timepreserve, half]' % self.layer_type)


class UpSample(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type

    def forward(self, x):
        if self.layer_type == 'none':
            return x
        elif self.layer_type == 'timepreserve':
            return F.interpolate(x, scale_factor=(2, 1), mode='nearest')
        elif self.layer_type == 'half':
            return F.interpolate(x, scale_factor=2, mode='nearest')
        else:
            raise RuntimeError('Got unexpected upsampletype %s, expected is [none, timepreserve, half]' % self.layer_type)


class ResBlk(nn.Module):
    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),
                 normalize=False, downsample='none'):
        super().__init__()
        self.actv = actv
        self.normalize = normalize
        self.downsample = DownSample(downsample)
        self.downsample_res = LearnedDownSample(downsample, dim_in)
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out)

    def _build_weights(self, dim_in, dim_out):
        self.conv1 = spectral_norm(nn.Conv2d(dim_in, dim_in, 3, 1, 1))
        self.conv2 = spectral_norm(nn.Conv2d(dim_in, dim_out, 3, 1, 1))
        if self.normalize:
            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)
            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)
        if self.learned_sc:
            self.conv1x1 = spectral_norm(nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False))

    def _shortcut(self, x):
        if self.learned_sc:
            x = self.conv1x1(x)
        if self.downsample:
            x = self.downsample(x)
        return x

    def _residual(self, x):
        if self.normalize:
            x = self.norm1(x)
        x = self.actv(x)
        x = self.conv1(x)
        x = self.downsample_res(x)
        if self.normalize:
            x = self.norm2(x)
        x = self.actv(x)
        x = self.conv2(x)
        return x

    def forward(self, x):
        x = self._shortcut(x) + self._residual(x)
        return x / np.sqrt(2)  # unit variance

class LinearNorm(torch.nn.Module):
    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):
        super(LinearNorm, self).__init__()
        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)

        torch.nn.init.xavier_uniform_(
            self.linear_layer.weight,
            gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, x):
        return self.linear_layer(x)

class Discriminator2d(nn.Module):
    def __init__(self, dim_in=48, num_domains=1, max_conv_dim=384, repeat_num=4):
        super().__init__()
        blocks = []
        blocks += [spectral_norm(nn.Conv2d(1, dim_in, 3, 1, 1))]

        for lid in range(repeat_num):
            dim_out = min(dim_in*2, max_conv_dim)
            blocks += [ResBlk(dim_in, dim_out, downsample='half')]
            dim_in = dim_out

        blocks += [nn.LeakyReLU(0.2)]
        blocks += [spectral_norm(nn.Conv2d(dim_out, dim_out, 5, 1, 0))]
        blocks += [nn.LeakyReLU(0.2)]
        blocks += [nn.AdaptiveAvgPool2d(1)]
        blocks += [spectral_norm(nn.Conv2d(dim_out, num_domains, 1, 1, 0))]
        self.main = nn.Sequential(*blocks)

    def get_feature(self, x):
        features = []
        for l in self.main:
            x = l(x)
            features.append(x) 
        out = features[-1]
        out = out.view(out.size(0), -1)  # (batch, num_domains)
        return out, features

    def forward(self, x):
        out, features = self.get_feature(x)
        out = out.squeeze()  # (batch)
        return out, features

class ResBlk1d(nn.Module):
    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),
                 normalize=False, downsample='none', dropout_p=0.2):
        super().__init__()
        self.actv = actv
        self.normalize = normalize
        self.downsample_type = downsample
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out)
        self.dropout_p = dropout_p
        
        if self.downsample_type == 'none':
            self.pool = nn.Identity()
        else:
            self.pool = weight_norm(nn.Conv1d(dim_in, dim_in, kernel_size=3, stride=2, groups=dim_in, padding=1))

    def _build_weights(self, dim_in, dim_out):
        self.conv1 = weight_norm(nn.Conv1d(dim_in, dim_in, 3, 1, 1))
        self.conv2 = weight_norm(nn.Conv1d(dim_in, dim_out, 3, 1, 1))
        if self.normalize:
            self.norm1 = nn.InstanceNorm1d(dim_in, affine=True)
            self.norm2 = nn.InstanceNorm1d(dim_in, affine=True)
        if self.learned_sc:
            self.conv1x1 = weight_norm(nn.Conv1d(dim_in, dim_out, 1, 1, 0, bias=False))

    def downsample(self, x):
        if self.downsample_type == 'none':
            return x
        else:
            if x.shape[-1] % 2 != 0:
                x = torch.cat([x, x[..., -1].unsqueeze(-1)], dim=-1)
            return F.avg_pool1d(x, 2)

    def _shortcut(self, x):
        if self.learned_sc:
            x = self.conv1x1(x)
        x = self.downsample(x)
        return x

    def _residual(self, x):
        if self.normalize:
            x = self.norm1(x)
        x = self.actv(x)
        x = F.dropout(x, p=self.dropout_p, training=self.training)
        
        x = self.conv1(x)
        x = self.pool(x)
        if self.normalize:
            x = self.norm2(x)
            
        x = self.actv(x)
        x = F.dropout(x, p=self.dropout_p, training=self.training)
        
        x = self.conv2(x)
        return x

    def forward(self, x):
        x = self._shortcut(x) + self._residual(x)
        return x / np.sqrt(2)  # unit variance

class LayerNorm(nn.Module):
    def __init__(self, channels, eps=1e-5):
        super().__init__()
        self.channels = channels
        self.eps = eps

        self.gamma = nn.Parameter(torch.ones(channels))
        self.beta = nn.Parameter(torch.zeros(channels))

    def forward(self, x):
        x = x.transpose(1, -1)
        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)
        return x.transpose(1, -1)
    
class TextEncoder(nn.Module):
    def __init__(self, channels, kernel_size, depth, n_symbols, actv=nn.LeakyReLU(0.2)):
        super().__init__()
        self.embedding = nn.Embedding(n_symbols, channels)

        padding = (kernel_size - 1) // 2
        self.cnn = nn.ModuleList()
        for _ in range(depth):
            self.cnn.append(nn.Sequential(
                weight_norm(nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=padding)),
                LayerNorm(channels),
                actv,
                nn.Dropout(0.2),
            ))
        # self.cnn = nn.Sequential(*self.cnn)

        self.lstm = nn.LSTM(channels, channels//2, 1, batch_first=True, bidirectional=True)

    def forward(self, x, input_lengths, m):
        x = self.embedding(x)  # [B, T, emb]
        x = x.transpose(1, 2)  # [B, emb, T]
        m = m.to(input_lengths.device).unsqueeze(1)
        x.masked_fill_(m, 0.0)
        
        for c in self.cnn:
            x = c(x)
            x.masked_fill_(m, 0.0)
            
        x = x.transpose(1, 2)  # [B, T, chn]

        input_lengths = input_lengths.cpu().numpy()
        x = nn.utils.rnn.pack_padded_sequence(
            x, input_lengths, batch_first=True, enforce_sorted=False)

        self.lstm.flatten_parameters()
        x, _ = self.lstm(x)
        x, _ = nn.utils.rnn.pad_packed_sequence(
            x, batch_first=True)
                
        x = x.transpose(-1, -2)
        x_pad = torch.zeros([x.shape[0], x.shape[1], m.shape[-1]])

        x_pad[:, :, :x.shape[-1]] = x
        x = x_pad.to(x.device)
        
        x.masked_fill_(m, 0.0)
        
        return x

    def inference(self, x):
        x = self.embedding(x)
        x = x.transpose(1, 2)
        x = self.cnn(x)
        x = x.transpose(1, 2)
        self.lstm.flatten_parameters()
        x, _ = self.lstm(x)
        return x
    
    def length_to_mask(self, lengths):
        mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)
        mask = torch.gt(mask+1, lengths.unsqueeze(1))
        return mask



class AdaIN1d(nn.Module):
    def __init__(self, style_dim, num_features):
        super().__init__()
        self.norm = nn.InstanceNorm1d(num_features, affine=False)
        self.fc = nn.Linear(style_dim, num_features*2)

    def forward(self, x, s):
        h = self.fc(s)
        h = h.view(h.size(0), h.size(1), 1)
        gamma, beta = torch.chunk(h, chunks=2, dim=1)
        return (1 + gamma) * self.norm(x) + beta

class UpSample1d(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type

    def forward(self, x):
        if self.layer_type == 'none':
            return x
        else:
            return F.interpolate(x, scale_factor=2, mode='nearest')

class AdainResBlk1d(nn.Module):
    def __init__(self, dim_in, dim_out, style_dim=64, actv=nn.LeakyReLU(0.2),
                 upsample='none', dropout_p=0.0):
        super().__init__()
        self.actv = actv
        self.upsample_type = upsample
        self.upsample = UpSample1d(upsample)
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out, style_dim)
        self.dropout = nn.Dropout(dropout_p)
        
        if upsample == 'none':
            self.pool = nn.Identity()
        else:
            self.pool = weight_norm(nn.ConvTranspose1d(dim_in, dim_in, kernel_size=3, stride=2, groups=dim_in, padding=1, output_padding=1))
        
        
    def _build_weights(self, dim_in, dim_out, style_dim):
        self.conv1 = weight_norm(nn.Conv1d(dim_in, dim_out, 3, 1, 1))
        self.conv2 = weight_norm(nn.Conv1d(dim_out, dim_out, 3, 1, 1))
        self.norm1 = AdaIN1d(style_dim, dim_in)
        self.norm2 = AdaIN1d(style_dim, dim_out)
        if self.learned_sc:
            self.conv1x1 = weight_norm(nn.Conv1d(dim_in, dim_out, 1, 1, 0, bias=False))

    def _shortcut(self, x):
        x = self.upsample(x)
        if self.learned_sc:
            x = self.conv1x1(x)
        return x

    def _residual(self, x, s):
        x = self.norm1(x, s)
        x = self.actv(x)
        x = self.pool(x)
        x = self.conv1(self.dropout(x))
        x = self.norm2(x, s)
        x = self.actv(x)
        x = self.conv2(self.dropout(x))
        return x

    def forward(self, x, s):
        out = self._residual(x, s)
        out = (out + self._shortcut(x)) / np.sqrt(2)
        return out
    
class AdaLayerNorm(nn.Module):
    def __init__(self, style_dim, channels, eps=1e-5):
        super().__init__()
        self.channels = channels
        self.eps = eps

        self.fc = nn.Linear(style_dim, channels*2)

    def forward(self, x, s):
        x = x.transpose(-1, -2)
        x = x.transpose(1, -1)
                
        h = self.fc(s)
        h = h.view(h.size(0), h.size(1), 1)
        gamma, beta = torch.chunk(h, chunks=2, dim=1)
        gamma, beta = gamma.transpose(1, -1), beta.transpose(1, -1)
        
        
        x = F.layer_norm(x, (self.channels,), eps=self.eps)
        x = (1 + gamma) * x + beta
        return x.transpose(1, -1).transpose(-1, -2)

class ProsodyPredictor(nn.Module):

    def __init__(self, style_dim, d_hid, nlayers, max_dur=50, dropout=0.1):
        super().__init__() 
        
        self.text_encoder = DurationEncoder(sty_dim=style_dim, 
                                            d_model=d_hid,
                                            nlayers=nlayers, 
                                            dropout=dropout)

        self.lstm = nn.LSTM(d_hid + style_dim, d_hid // 2, 1, batch_first=True, bidirectional=True)
        self.duration_proj = LinearNorm(d_hid, max_dur)
        
        self.shared = nn.LSTM(d_hid + style_dim, d_hid // 2, 1, batch_first=True, bidirectional=True)
        self.F0 = nn.ModuleList()
        self.F0.append(AdainResBlk1d(d_hid, d_hid, style_dim, dropout_p=dropout))
        self.F0.append(AdainResBlk1d(d_hid, d_hid // 2, style_dim, upsample=True, dropout_p=dropout))
        self.F0.append(AdainResBlk1d(d_hid // 2, d_hid // 2, style_dim, dropout_p=dropout))

        self.N = nn.ModuleList()
        self.N.append(AdainResBlk1d(d_hid, d_hid, style_dim, dropout_p=dropout))
        self.N.append(AdainResBlk1d(d_hid, d_hid // 2, style_dim, upsample=True, dropout_p=dropout))
        self.N.append(AdainResBlk1d(d_hid // 2, d_hid // 2, style_dim, dropout_p=dropout))
        
        self.F0_proj = nn.Conv1d(d_hid // 2, 1, 1, 1, 0)
        self.N_proj = nn.Conv1d(d_hid // 2, 1, 1, 1, 0)


    def forward(self, texts, style, text_lengths, alignment, m):
        d = self.text_encoder(texts, style, text_lengths, m)
        
        batch_size = d.shape[0]
        text_size = d.shape[1]
        
        # predict duration
        input_lengths = text_lengths.cpu().numpy()
        x = nn.utils.rnn.pack_padded_sequence(
            d, input_lengths, batch_first=True, enforce_sorted=False)
        
        m = m.to(text_lengths.device).unsqueeze(1)
        
        self.lstm.flatten_parameters()
        x, _ = self.lstm(x)
        x, _ = nn.utils.rnn.pad_packed_sequence(
            x, batch_first=True)
        
        x_pad = torch.zeros([x.shape[0], m.shape[-1], x.shape[-1]])

        x_pad[:, :x.shape[1], :] = x
        x = x_pad.to(x.device)
                
        duration = self.duration_proj(nn.functional.dropout(x, 0.5, training=self.training))
        
        en = (d.transpose(-1, -2) @ alignment)

        return duration.squeeze(-1), en
    
    def F0Ntrain(self, x, s):
        x, _ = self.shared(x.transpose(-1, -2))
        
        F0 = x.transpose(-1, -2)
        for block in self.F0:
            F0 = block(F0, s)
        F0 = self.F0_proj(F0)

        N = x.transpose(-1, -2)
        for block in self.N:
            N = block(N, s)
        N = self.N_proj(N)
        
        return F0.squeeze(1), N.squeeze(1)
    
    def length_to_mask(self, lengths):
        mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)
        mask = torch.gt(mask+1, lengths.unsqueeze(1))
        return mask
    
class DurationEncoder(nn.Module):

    def __init__(self, sty_dim, d_model, nlayers, dropout=0.1):
        super().__init__()
        self.lstms = nn.ModuleList()
        for _ in range(nlayers):
            self.lstms.append(nn.LSTM(d_model + sty_dim, 
                                 d_model // 2, 
                                 num_layers=1, 
                                 batch_first=True, 
                                 bidirectional=True, 
                                 dropout=dropout))
            self.lstms.append(AdaLayerNorm(sty_dim, d_model))
        
        
        self.dropout = dropout
        self.d_model = d_model
        self.sty_dim = sty_dim

    def forward(self, x, style, text_lengths, m):
        masks = m.to(text_lengths.device)
        
        x = x.permute(2, 0, 1)
        s = style.expand(x.shape[0], x.shape[1], -1)
        x = torch.cat([x, s], axis=-1)
        x.masked_fill_(masks.unsqueeze(-1).transpose(0, 1), 0.0)
                
        x = x.transpose(0, 1)
        input_lengths = text_lengths.cpu().numpy()
        x = x.transpose(-1, -2)
        
        for block in self.lstms:
            if isinstance(block, AdaLayerNorm):
                x = block(x.transpose(-1, -2), style).transpose(-1, -2)
                x = torch.cat([x, s.permute(1, -1, 0)], axis=1)
                x.masked_fill_(masks.unsqueeze(-1).transpose(-1, -2), 0.0)
            else:
                x = x.transpose(-1, -2)
                x = nn.utils.rnn.pack_padded_sequence(
                    x, input_lengths, batch_first=True, enforce_sorted=False)
                block.flatten_parameters()
                x, _ = block(x)
                x, _ = nn.utils.rnn.pad_packed_sequence(
                    x, batch_first=True)
                x = F.dropout(x, p=self.dropout, training=self.training)
                x = x.transpose(-1, -2)
                
                x_pad = torch.zeros([x.shape[0], x.shape[1], m.shape[-1]])

                x_pad[:, :, :x.shape[-1]] = x
                x = x_pad.to(x.device)
        
        return x.transpose(-1, -2)
    
    def inference(self, x, style):
        x = self.embedding(x.transpose(-1, -2)) * np.sqrt(self.d_model)
        style = style.expand(x.shape[0], x.shape[1], -1)
        x = torch.cat([x, style], axis=-1)
        src = self.pos_encoder(x)
        output = self.transformer_encoder(src).transpose(0, 1)
        return output
    
    def length_to_mask(self, lengths):
        mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)
        mask = torch.gt(mask+1, lengths.unsqueeze(1))
        return mask

# https://github.com/yl4579/StyleTTS2/blob/main/utils.py
def recursive_munch(d):
    if isinstance(d, dict):
        return Munch((k, recursive_munch(v)) for k, v in d.items())
    elif isinstance(d, list):
        return [recursive_munch(v) for v in d]
    else:
        return d

def build_model(path, device):
    config = Path(__file__).parent / 'config.json'
    assert config.exists(), f'Config path incorrect: config.json not found at {config}'
    with open(config, 'r') as r:
        args = recursive_munch(json.load(r))
    assert args.decoder.type == 'istftnet', f'Unknown decoder type: {args.decoder.type}'
    decoder = Decoder(dim_in=args.hidden_dim, style_dim=args.style_dim, dim_out=args.n_mels,
            resblock_kernel_sizes = args.decoder.resblock_kernel_sizes,
            upsample_rates = args.decoder.upsample_rates,
            upsample_initial_channel=args.decoder.upsample_initial_channel,
            resblock_dilation_sizes=args.decoder.resblock_dilation_sizes,
            upsample_kernel_sizes=args.decoder.upsample_kernel_sizes,
            gen_istft_n_fft=args.decoder.gen_istft_n_fft, gen_istft_hop_size=args.decoder.gen_istft_hop_size)
    text_encoder = TextEncoder(channels=args.hidden_dim, kernel_size=5, depth=args.n_layer, n_symbols=args.n_token)
    predictor = ProsodyPredictor(style_dim=args.style_dim, d_hid=args.hidden_dim, nlayers=args.n_layer, max_dur=args.max_dur, dropout=args.dropout)
    bert = load_plbert()
    bert_encoder = nn.Linear(bert.config.hidden_size, args.hidden_dim)
    for parent in [bert, bert_encoder, predictor, decoder, text_encoder]:
        for child in parent.children():
            if isinstance(child, nn.RNNBase):
                child.flatten_parameters()
    model = Munch(
        bert=bert.to(device).eval(),
        bert_encoder=bert_encoder.to(device).eval(),
        predictor=predictor.to(device).eval(),
        decoder=decoder.to(device).eval(),
        text_encoder=text_encoder.to(device).eval(),
    )
    for key, state_dict in torch.load(path, map_location='cpu', weights_only=True)['net'].items():
        assert key in model, key
        try:
            model[key].load_state_dict(state_dict)
        except:
            state_dict = {k[7:]: v for k, v in state_dict.items()}
            model[key].load_state_dict(state_dict, strict=False)
    return model


#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\plbert.py
# https://github.com/yl4579/StyleTTS2/blob/main/Utils/PLBERT/util.py
from transformers import AlbertConfig, AlbertModel

class CustomAlbert(AlbertModel):
    def forward(self, *args, **kwargs):
        # Call the original forward method
        outputs = super().forward(*args, **kwargs)
        # Only return the last_hidden_state
        return outputs.last_hidden_state

def load_plbert():
    plbert_config = {'vocab_size': 178, 'hidden_size': 768, 'num_attention_heads': 12, 'intermediate_size': 2048, 'max_position_embeddings': 512, 'num_hidden_layers': 12, 'dropout': 0.1}
    albert_base_configuration = AlbertConfig(**plbert_config)
    bert = CustomAlbert(albert_base_configuration)
    return bert


#### file: C:\Users\mateo\Desktop\pdf-narrator\Kokoro\README.md
---
license: apache-2.0
language:
- en
base_model:
- yl4579/StyleTTS2-LJSpeech
pipeline_tag: text-to-speech
---
â�¤ï¸� Kokoro Discord Server: https://discord.gg/QuGxSWBfQy

<audio controls><source src="https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/demo/HEARME.wav" type="audio/wav"></audio>

**Kokoro** is a frontier TTS model for its size of **82 million parameters** (text in/audio out).

On 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision along with 2 voicepacks (Bella and Sarah), all under an Apache 2.0 license.

As of 28 Dec 2024, **8 unique Voicepacks have been released**: 2F 2M each for American and British English.

At the time of release, Kokoro v0.19 was the #1ðŸ¥‡ ranked model in [TTS Spaces Arena](https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena). Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data:
1. **Kokoro v0.19: 82M params, Apache, trained on <100 hours of audio, for <20 epochs**
2. XTTS v2: 467M, CPML, >10k hours
3. Edge TTS: Microsoft, proprietary
4. MetaVoice: 1.2B, Apache, 100k hours
5. Parler Mini: 880M, Apache, 45k hours
6. Fish Speech: ~500M, CC-BY-NC-SA, 1M hours

Kokoro's ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected.

You can find a hosted demo at [hf.co/spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS).

### Usage

The following can be run in a single cell on [Google Colab](https://colab.research.google.com/).
```py
# 1ï¸�âƒ£ Install dependencies silently
!git clone https://huggingface.co/hexgrad/Kokoro-82M
%cd Kokoro-82M
!apt-get -qq -y install espeak-ng > /dev/null 2>&1
!pip install -q phonemizer torch transformers scipy munch

# 2ï¸�âƒ£ Build the model and load the default voicepack
from models import build_model
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
MODEL = build_model('kokoro-v0_19.pth', device)
VOICE_NAME = [
    'af', # Default voice is a 50-50 mix of af_bella & af_sarah
    'af_bella', 'af_sarah', 'am_adam', 'am_michael',
    'bf_emma', 'bf_isabella', 'bm_george', 'bm_lewis',
][0]
VOICEPACK = torch.load(f'voices/{VOICE_NAME}.pt', weights_only=True).to(device)
print(f'Loaded voice: {VOICE_NAME}')

# 3ï¸�âƒ£ Call generate, which returns a 24khz audio waveform and a string of output phonemes
from kokoro import generate
text = "How could I know? It's an unanswerable question. Like asking an unborn child if they'll lead a good life. They haven't even been born."
audio, out_ps = generate(MODEL, text, VOICEPACK, lang=VOICE_NAME[0])
# Language is determined by the first letter of the VOICE_NAME:
# ðŸ‡ºðŸ‡¸ 'a' => American English => en-us
# ðŸ‡¬ðŸ‡§ 'b' => British English => en-gb

# 4ï¸�âƒ£ Display the 24khz audio and print the output phonemes
from IPython.display import display, Audio
display(Audio(data=audio, rate=24000, autoplay=True))
print(out_ps)
```
The inference code was quickly hacked together on Christmas Day. It is not clean code and leaves a lot of room for improvement. If you'd like to contribute, feel free to open a PR.

### Model Facts

No affiliation can be assumed between parties on different lines.

**Architecture:**
- StyleTTS 2: https://arxiv.org/abs/2306.07691
- ISTFTNet: https://arxiv.org/abs/2203.02395
- Decoder only: no diffusion, no encoder release

**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2

**Trained by**: `@rzvzn` on Discord

**Supported Languages:** American English, British English

**Model SHA256 Hash:** `3b0c392f87508da38fad3a2f9d94c359f1b657ebd2ef79f9d56d69503e470b0a`

### Releases
- 25 Dec 2024: Model v0.19, `af_bella`, `af_sarah`
- 26 Dec 2024: `am_adam`, `am_michael`
- 28 Dec 2024: `bf_emma`, `bf_isabella`, `bm_george`, `bm_lewis`

### Licenses
- Apache 2.0 weights in this repository
- MIT inference code in [spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS) adapted from [yl4579/StyleTTS2](https://github.com/yl4579/StyleTTS2)
- GPLv3 dependency in [espeak-ng](https://github.com/espeak-ng/espeak-ng)

The inference code was originally MIT licensed by the paper author. Note that this card applies only to this model, Kokoro. Original models published by the paper author can be found at [hf.co/yl4579](https://huggingface.co/yl4579).

### Evaluation

**Metric:** Elo rating

**Leaderboard:** [hf.co/spaces/Pendrokar/TTS-Spaces-Arena](https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena)

![TTS-Spaces-Arena-25-Dec-2024](demo/TTS-Spaces-Arena-25-Dec-2024.png)

The voice ranked in the Arena is a 50-50 mix of Bella and Sarah. For your convenience, this mix is included in this repository as `af.pt`, but you can trivially reproduce it like this:

```py
import torch
bella = torch.load('voices/af_bella.pt', weights_only=True)
sarah = torch.load('voices/af_sarah.pt', weights_only=True)
af = torch.mean(torch.stack([bella, sarah]), dim=0)
assert torch.equal(af, torch.load('voices/af.pt', weights_only=True))
```

### Training Details

**Compute:** Kokoro was trained on A100 80GB vRAM instances rented from [Vast.ai](https://cloud.vast.ai/?ref_id=79907) (referral link). Vast was chosen over other compute providers due to its competitive on-demand hourly rates. The average hourly cost for the A100 80GB vRAM instances used for training was below $1/hr per GPU, which was around half the quoted rates from other providers at the time.

**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:
- Public domain audio
- Audio licensed under Apache, MIT, etc
- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>
[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>
[2] No synthetic audio from open TTS models or "custom voice clones"

**Epochs:** Less than **20 epochs**

**Total Dataset Size:** Less than **100 hours** of audio

### Limitations

Kokoro v0.19 is limited in some specific ways, due to its training set and/or architecture:
- [Data] Lacks voice cloning capability, likely due to small <100h training set
- [Arch] Relies on external g2p (espeak-ng), which introduces a class of g2p failure modes
- [Data] Training dataset is mostly long-form reading and narration, not conversation
- [Arch] At 82M params, Kokoro almost certainly falls to a well-trained 1B+ param diffusion transformer, or a many-billion-param MLLM like GPT-4o / Gemini 2.0 Flash
- [Data] Multilingual capability is architecturally feasible, but training data is mostly English

Refer to the [Philosophy discussion](https://huggingface.co/hexgrad/Kokoro-82M/discussions/5) to better understand these limitations.

**Will the other voicepacks be released?** There is currently no release date scheduled for the other voicepacks, but in the meantime you can try them in the hosted demo at [hf.co/spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS).

### Acknowledgements
- [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2
- [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena

### Model Card Contact

`@rzvzn` on Discord. Server invite: https://discord.gg/QuGxSWBfQy

<img src="https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg" width="400" alt="kokoro" />
